<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Simple MPI parallelism #  In this exercise we&rsquo;re going to compute an approximation to the value of π using a simple Monte Carlo method. We do this by noticing that if we randomly throw darts at a square, the fraction of the time they will fall within the incircle approaches π.
Consider a square with side-length $2r$ and an inscribed circle with radius $r$.
 Square with inscribed circle"><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Calculating π"><meta property="og:description" content="Simple MPI parallelism #  In this exercise we&rsquo;re going to compute an approximation to the value of π using a simple Monte Carlo method. We do this by noticing that if we randomly throw darts at a square, the fraction of the time they will fall within the incircle approaches π.
Consider a square with side-length $2r$ and an inscribed circle with radius $r$.
 Square with inscribed circle"><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/comp4187/exercises/parallel/pi/"><meta property="article:modified_time" content="2024-02-14T14:35:09+00:00"><title>Calculating π | COMP4187 – Parallel Scientific Computing II</title><link rel=manifest href=/comp4187/manifest.json><link rel=icon href=/comp4187/favicon.png type=image/x-icon><link rel=stylesheet href=/comp4187/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/comp4187/logo.svg alt=Logo><h2><a href=/comp4187>COMP4187 – Parallel Scientific Computing II</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/comp4187/setup/jupyter/>Jupyter</a></li><li><a href=/comp4187/setup/mpi/>MPI</a></li></ul></li><li><span>Exercises</span><ul><li><span>Parallel</span><ul><li><a href=/comp4187/exercises/parallel/hello/>Parallel Hello World</a></li><li><a href=/comp4187/exercises/parallel/pi/ class=active>Calculating π</a></li><li><a href=/comp4187/exercises/parallel/domain-decomp-simple/>1-D domain decomposition</a></li><li><a href=/comp4187/exercises/parallel/pingpong/>Ping-pong latency</a></li></ul></li><li><a href=/comp4187/exercises/coarse-grid/>Coarse Grid Operator</a></li><li><a href=/comp4187/exercises/finite-differences/>Finite Differences</a></li><li><a href=/comp4187/exercises/two-grid/>Two-Grid Iteration</a></li><li><a href=/comp4187/exercises/norms/>Norms</a></li></ul></li><li><span>Notes</span><ul><li><span>Lectures: Numerics</span><ul><li><a href=/comp4187/lectures/numerics/lecture1/>Lecture 1: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture2/>Lecture 2: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture3/>Lecture 3: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture4/>Lecture 4: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture5/>Lecture 5: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture6/>Lecture 6: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture7/>Lecture 7: Linear Solvers</a></li><li><a href=/comp4187/lectures/numerics/lecture8/>Lecture 8: Linear Solvers</a></li><li><a href=/comp4187/lectures/numerics/lecture9/>Lecture 9: Multigrid</a></li></ul></li><li><a href=/comp4187/lectures/mpi/>MPI</a><ul><li><a href=/comp4187/lectures/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/comp4187/lectures/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/comp4187/lectures/mpi/collectives/>Collectives</a></li><li><a href=/comp4187/lectures/mpi/advanced/>Advanced topics</a></li><li><a href=/comp4187/lectures/mpi/petsc4py/>PETSc and petsc4py</a></li><li><a href=/comp4187/lectures/mpi/live-notes/>Term 2: live lecture notes</a></li></ul></li></ul></li><li><a href=/comp4187/coursework/>Coursework 1: Euler-Bernoulli Beam Theory</a></li><li><a href=/comp4187/coursework2/>Coursework 2: multigrid solvers</a></li><li><a href=/comp4187/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/comp4187/past-editions/2020-21/term1/>Term 1: numerics</a></li><li><a href=/comp4187/past-editions/2020-21/term2/>Term 2: parallel computing</a></li><li><a href=/comp4187/past-editions/2020-21/coursework/>Coursework: a 3D multigrid solver</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/comp4187/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Calculating π</strong>
<label for=toc-control><img src=/comp4187/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#simple-mpi-parallelism>Simple MPI parallelism</a><ul><li><a href=#obtaining-the-code>Obtaining the code</a></li><li><a href=#parallelisation-with-mpi>Parallelisation with MPI</a><ul><li><a href=#parallelising-the-random-number-generation>Parallelising the random number generation</a></li><li><a href=#benchmarking>Benchmarking</a></li></ul></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=simple-mpi-parallelism>Simple MPI parallelism
<a class=anchor href=#simple-mpi-parallelism>#</a></h1><p>In this exercise we&rsquo;re going to compute an approximation to the value
of π using a simple Monte Carlo method. We do this by noticing that if
we randomly throw darts at a square, the fraction of the time they
will fall within the incircle approaches π.</p><p>Consider a square with side-length $2r$ and an inscribed circle
with radius $r$.</p><figure style=width:40%><img class=scaled src=https://teaching.wence.uk/comp4187/images/manual/square-circle.svg alt="Square with inscribed circle"><figcaption><p>Square with inscribed circle</p></figcaption></figure><p>The ratio of areas is</p><p>$$
\frac{A_\text{circle}}{A_\text{square}} = \frac{\pi r^2}{4 r^2} = \frac{\pi}{4}.
$$</p><p>If we therefore draw $X$ uniformly at random from the distribution
$\mathcal{U}(0, r) \times \mathcal{U}(0, r)$, then the
probability that \(X\) is in the circle is</p><p>$$
p_\text{in} = \frac{\pi}{4}.
$$</p><p>We can therefore approximate π by picking $N_\text{total}$ random
points and counting the number, $N_\text{circle}$, that fall within the
circle</p><p>$$
\pi_\text{numerical} = 4 \frac{N_\text{circle}}{N_\text{total}}
$$</p><h2 id=obtaining-the-code>Obtaining the code
<a class=anchor href=#obtaining-the-code>#</a></h2><p>The code for this exercise lives in the <code>code/parallel/pi/</code>
subdirectory in the <a href=https://github.com/wenceorg/comp4187>repository</a>, as <code>pi.py</code>.</p><details><summary>Working from the repository</summary><div class=markdown-inner>I recommend working on a branch in your clone of the repository, so
that you can commit any changes you make and experiments you do.</div></details><p>I provide a simple serial implementation that uses numpy to generate
the random numbers.</p><blockquote class=exercise><h3>Exercise</h3><span><p>Adapt the code so that it runs for a range of different choices of the
number of samples, <code>N</code>. Plot the error in the estimated value of $\pi$
as a function of <code>N</code>.</p><p>What relationship do you observe between the accuracy of the
approximate result and <code>N</code>?</p><details><summary>Hint</summary><div class=markdown-inner>A double-precision approximation to $\pi$ is available as <code>numpy.pi</code>.</div></details></span></blockquote><h2 id=parallelisation-with-mpi>Parallelisation with MPI
<a class=anchor href=#parallelisation-with-mpi>#</a></h2><p>We&rsquo;re now going to parallelise this computation with MPI.</p><blockquote class="book-hint info"><span><p>If you&rsquo;re running on Hamilton don&rsquo;t forget to load the right modules</p><pre><code>python/3.6.8
gcc/8.2.0
intelmpi/gcc/2019.6
</code></pre></span></blockquote><p>The code already imports <code>mpi4py</code>, but does not distribute the work.</p><blockquote class=exercise><h3>Exercise</h3><span><p>Adapt the <code>run</code> function so that the total samples are (approximately)
evenly distributed between all the ranks in the given communicator.</p><p>You&rsquo;ll now have each process computing a partial answer, so combine
them with
<a href=https://rookiehpc.com/mpi/docs/mpi_allreduce.php><code>MPI_Allreduce</code></a>.</p></span></blockquote><h3 id=parallelising-the-random-number-generation>Parallelising the random number generation
<a class=anchor href=#parallelising-the-random-number-generation>#</a></h3><p>Running this code in parallel presents us with a slight problem. We
need to think about how to provide statistically independent random
number streams on the different processes.</p><p>Fortunately, modern versions of numpy have us covered. Their
documentation describes how to obtain <a href=https://numpy.org/doc/stable/reference/random/parallel.html>random numbers in
parallel</a>.</p><blockquote class=exercise><h3>Exercise</h3><span>Replace the use of the <code>default_rng</code> generator with a Generator that
will produce a different stream on each process. Remember that the
<code>comm.rank</code> is unique to each process in the communicator.</span></blockquote><h3 id=benchmarking>Benchmarking
<a class=anchor href=#benchmarking>#</a></h3><p>We&rsquo;ll now briefly look at how this code scales, by carrying out some
simple strong and weak scaling tests.</p><blockquote class=exercise><h3>Exercise</h3><span><p>Use <code>MPI.Wtime()</code> to measure the length of time the <code>run</code> function
takes on each process.</p><p>Use the maximum over all processes for your plots.</p><ol><li>Produce a strong scaling plot using a total of $N=10^8$ points.</li><li>Produce weak scaling plots using $N=10^4$, $N=10^5$, $N=10^6$, and
$N=10^7$ points per process.</li></ol><p>What observations do you make about the scaling behaviour?</p></span></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/comp4187/commit/9271858aa631161252031b9cd38d4da1c5c802a0 title="Last modified by Lawrence Mitchell | February 14, 2024" target=_blank rel=noopener><img src=/comp4187/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 14, 2024</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/comp4187/edit/main/site/content/exercises/parallel/pi.md target=_blank rel=noopener><img src=/comp4187/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence@wence.uk>Lawrence Mitchell</a>, <a href=https://annereinarz.github.io>Anne Reinarz</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/comp4187/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#simple-mpi-parallelism>Simple MPI parallelism</a><ul><li><a href=#obtaining-the-code>Obtaining the code</a></li><li><a href=#parallelisation-with-mpi>Parallelisation with MPI</a><ul><li><a href=#parallelising-the-random-number-generation>Parallelising the random number generation</a></li><li><a href=#benchmarking>Benchmarking</a></li></ul></li></ul></li></ul></nav></aside></main></body></html>