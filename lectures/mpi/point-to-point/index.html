<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Pairwise message exchange #  The simplest form of communication in MPI is a pairwise exchange of a message between two processes.
In MPI, communication via messages is two-sided1. That is, for every message one process sends, there must be a matching receive call by another process.
 Cartoon of sending a message between two processes
  We need to fill in some details
 How will we describe &ldquo;data&rdquo; How will we identify processes How will the receiver know which message to put where?"><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Point-to-point messaging in MPI"><meta property="og:description" content="Pairwise message exchange #  The simplest form of communication in MPI is a pairwise exchange of a message between two processes.
In MPI, communication via messages is two-sided1. That is, for every message one process sends, there must be a matching receive call by another process.
 Cartoon of sending a message between two processes
  We need to fill in some details
 How will we describe &ldquo;data&rdquo; How will we identify processes How will the receiver know which message to put where?"><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/comp4187/lectures/mpi/point-to-point/"><meta property="article:modified_time" content="2024-02-14T14:35:09+00:00"><title>Point-to-point messaging in MPI | COMP4187 – Parallel Scientific Computing II</title><link rel=manifest href=/comp4187/manifest.json><link rel=icon href=/comp4187/favicon.png type=image/x-icon><link rel=stylesheet href=/comp4187/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/comp4187/logo.svg alt=Logo><h2><a href=/comp4187>COMP4187 – Parallel Scientific Computing II</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/comp4187/setup/jupyter/>Jupyter</a></li><li><a href=/comp4187/setup/mpi/>MPI</a></li></ul></li><li><span>Exercises</span><ul><li><span>Parallel</span><ul><li><a href=/comp4187/exercises/parallel/hello/>Parallel Hello World</a></li><li><a href=/comp4187/exercises/parallel/pi/>Calculating π</a></li><li><a href=/comp4187/exercises/parallel/domain-decomp-simple/>1-D domain decomposition</a></li><li><a href=/comp4187/exercises/parallel/pingpong/>Ping-pong latency</a></li></ul></li><li><a href=/comp4187/exercises/coarse-grid/>Coarse Grid Operator</a></li><li><a href=/comp4187/exercises/finite-differences/>Finite Differences</a></li><li><a href=/comp4187/exercises/two-grid/>Two-Grid Iteration</a></li><li><a href=/comp4187/exercises/norms/>Norms</a></li></ul></li><li><span>Notes</span><ul><li><span>Lectures: Numerics</span><ul><li><a href=/comp4187/lectures/numerics/lecture1/>Lecture 1: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture2/>Lecture 2: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture3/>Lecture 3: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture4/>Lecture 4: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture5/>Lecture 5: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture6/>Lecture 6: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture7/>Lecture 7: Linear Solvers</a></li><li><a href=/comp4187/lectures/numerics/lecture8/>Lecture 8: Linear Solvers</a></li><li><a href=/comp4187/lectures/numerics/lecture9/>Lecture 9: Multigrid</a></li></ul></li><li><a href=/comp4187/lectures/mpi/>MPI</a><ul><li><a href=/comp4187/lectures/mpi/point-to-point/ class=active>Point-to-point messaging in MPI</a></li><li><a href=/comp4187/lectures/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/comp4187/lectures/mpi/collectives/>Collectives</a></li><li><a href=/comp4187/lectures/mpi/advanced/>Advanced topics</a></li><li><a href=/comp4187/lectures/mpi/petsc4py/>PETSc and petsc4py</a></li><li><a href=/comp4187/lectures/mpi/live-notes/>Term 2: live lecture notes</a></li></ul></li></ul></li><li><a href=/comp4187/coursework/>Coursework 1: Euler-Bernoulli Beam Theory</a></li><li><a href=/comp4187/coursework2/>Coursework 2: multigrid solvers</a></li><li><a href=/comp4187/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/comp4187/past-editions/2020-21/term1/>Term 1: numerics</a></li><li><a href=/comp4187/past-editions/2020-21/term2/>Term 2: parallel computing</a></li><li><a href=/comp4187/past-editions/2020-21/coursework/>Coursework: a 3D multigrid solver</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/comp4187/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Point-to-point messaging in MPI</strong>
<label for=toc-control><img src=/comp4187/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#pairwise-message-exchange>Pairwise message exchange</a><ul><li><a href=#describing-the-data>Describing the data</a></li><li><a href=#identification-of-processes-and-distinguishing-messages>Identification of processes and distinguishing messages</a><ul><li><a href=#message-ordering>Message ordering</a></li></ul></li><li><a href=#when-are-sends-receives-complete>When are sends (receives) complete?</a></li><li><a href=#different-types-of-send-calls>Different types of send calls</a><ul><li><a href=#synchronous-send-mpi_ssend>Synchronous send: <code>MPI_Ssend</code></a></li><li><a href=#buffered-send-mpi_bsend>Buffered send <code>MPI_Bsend</code></a></li><li><a href=#i-dont-want-to-decide-mpi_send>I don&rsquo;t want to decide: <code>MPI_Send</code></a></li><li><a href=#a-concrete-example>A concrete example</a></li></ul></li><li><a href=#avoiding-deadlocks>Avoiding deadlocks</a><ul><li><a href=#pairwise-communication-mpi_sendrecv>Pairwise communication: <code>MPI_Sendrecv</code></a></li><li><a href=#non-blocking-communication>Non-blocking communication</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=pairwise-message-exchange>Pairwise message exchange
<a class=anchor href=#pairwise-message-exchange>#</a></h1><p>The simplest form of communication in MPI is a pairwise exchange of a
message between two processes.</p><p>In MPI, communication via messages is <em>two-sided</em><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. That is, for every
message one process sends, there must be a matching receive call by
another process.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/comp4187/images/manual/mpi-send-recv-cartoon.svg alt="Cartoon of sending a message between two processes"><figcaption><p>Cartoon of sending a message between two processes</p></figcaption></figure><p>We need to fill in some details</p><ol><li>How will we describe &ldquo;data&rdquo;</li><li>How will we identify processes</li><li>How will the receiver know which message to put where?</li><li>What does it mean for a send (or receive) to be complete?</li></ol><p>The C function signatures for basic, blocking send and receive are</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Send</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>dtype</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Recv</span><span style=color:#111>(</span><span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>dtype</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>src</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#111>MPI_Status</span> <span style=color:#f92672>*</span><span style=color:#111>status</span><span style=color:#111>);</span>
</code></pre></div><p>The mpi4py buffer-based signatures are:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Send</span><span style=color:#111>([</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>datatype</span><span style=color:#111>],</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#111>)</span>
<span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Recv</span><span style=color:#111>([</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>datatype</span><span style=color:#111>],</span> <span style=color:#111>src</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#111>)</span>
</code></pre></div><p>As long as you&rsquo;re using numpy arrays and sending contiguous pieces of
data, mpi4py can also do automatic <a href=https://mpi4py.readthedocs.io/en/stable/tutorial.html#tutorial>datatype
discovery</a>,
in which case you can just do <code>comm.Send(buffer, dest, tag)</code>.</p><p>We first note a few things about the interface, and then describe the
details. All input and output variables are as arguments to the
functions.</p><blockquote class="book-hint info"><span>mpi4py sets up the MPI library such that errors from the library
functions (reported as non-zero integer return values in the C
interface) raise exceptions in Python. See the documentation on <a href=https://mpi4py.readthedocs.io/en/stable/overview.html#error-handling>error
handling</a>.</span></blockquote><p>Let&rsquo;s look at how this works in more detail.</p><h2 id=describing-the-data>Describing the data
<a class=anchor href=#describing-the-data>#</a></h2><p>To provide the data, we pass a buffer we want to send from (receive
into). We describe how much data to send (receive) by providing a
<code>count</code> and a datatype. MPI datatypes are quite flexible, we will
start off only using builtin datatypes (for describing the basic
variable types that C supports). We show a list of the more common
ones below, see the section <a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node459.htm#Node459>Named Predefined Datatypes C
types</a>
in the MPI standard for the full list.</p><table><thead><tr><th align=left>Numpy type</th><th align=left>C type<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></th><th align=left>MPI_Datatype</th></tr></thead><tbody><tr><td align=left>np.int32</td><td align=left><code>int32_t</code></td><td align=left>MPI.INT32_T</td></tr><tr><td align=left>np.int64</td><td align=left><code>int64_t</code></td><td align=left>MPI.INT64_T</td></tr><tr><td align=left>np.float32</td><td align=left><code>float</code></td><td align=left>MPI.FLOAT</td></tr><tr><td align=left>np.float64</td><td align=left><code>double</code></td><td align=left>MPI.DOUBLE</td></tr></tbody></table><p>For example, to send a single double we would write:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>value</span> <span style=color:#f92672>=</span> <span style=color:#111>numpy</span><span style=color:#f92672>.</span><span style=color:#111>asarray</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>dtype</span><span style=color:#f92672>=</span><span style=color:#111>numpy</span><span style=color:#f92672>.</span><span style=color:#111>float64</span><span style=color:#111>)</span>
<span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Send</span><span style=color:#111>([</span><span style=color:#111>value</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>DOUBLE</span><span style=color:#111>],</span> <span style=color:#f92672>...</span><span style=color:#111>)</span>
</code></pre></div><p>To send the second and third integers from an array of integers</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>numbers</span> <span style=color:#f92672>=</span> <span style=color:#111>numpy</span><span style=color:#f92672>.</span><span style=color:#111>arange</span><span style=color:#111>(</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>dtype</span><span style=color:#f92672>=</span><span style=color:#111>numpy</span><span style=color:#f92672>.</span><span style=color:#111>int32</span><span style=color:#111>)</span>
<span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Send</span><span style=color:#111>([</span><span style=color:#111>numbers</span><span style=color:#111>[</span><span style=color:#ae81ff>1</span><span style=color:#111>:</span><span style=color:#ae81ff>3</span><span style=color:#111>],</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>INT32_T</span><span style=color:#111>],</span> <span style=color:#f92672>...</span><span style=color:#111>)</span>
</code></pre></div><p>Receiving works analogously, so to receive the two integers, this time
into the first two entries of a buffer</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>numbers</span> <span style=color:#f92672>=</span> <span style=color:#111>numpy</span><span style=color:#f92672>.</span><span style=color:#111>empty</span><span style=color:#111>(</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>dtype</span><span style=color:#f92672>=</span><span style=color:#111>numpy</span><span style=color:#f92672>.</span><span style=color:#111>int32</span><span style=color:#111>)</span>
<span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Recv</span><span style=color:#111>([</span><span style=color:#111>numbers</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>INT32_T</span><span style=color:#111>],</span> <span style=color:#f92672>...</span><span style=color:#111>)</span>
</code></pre></div><h2 id=identification-of-processes-and-distinguishing-messages>Identification of processes and distinguishing messages
<a class=anchor href=#identification-of-processes-and-distinguishing-messages>#</a></h2><p>The concept that groups together processes in an MPI program is a
<em>communicator</em>. To identify processes in a send (receive) we use their
<code>rank</code> in a particular communicator. As we saw previously, MPI starts
up and provides a communicator that contains all processes, namely
<code>MPI.COMM_WORLD</code>.</p><p>Suppose I further (for my application) want to distinguish messages
with the same datatype/count arguments. I can use the <em>tags</em> to do so.
A message sent with tag <code>N</code> will only be matched by a receive that
also has tag <code>N</code>. Often it doesn&rsquo;t matter that much what we use as a
tag, because we arrange our code so that they are not important.</p><p>So if I want to send to rank 1 in <code>MPI.COMM_WORLD</code>, I write</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>comm</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>COMM_WORLD</span>
<span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Send</span><span style=color:#111>(</span><span style=color:#f92672>...</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#f92672>=</span><span style=color:#ae81ff>100</span><span style=color:#111>)</span>
</code></pre></div><p>Rank 1 can receive this message with:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>comm</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>COMM_WORLD</span>
<span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Recv</span><span style=color:#111>(</span><span style=color:#f92672>...</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#f92672>=</span><span style=color:#ae81ff>100</span><span style=color:#111>)</span>
</code></pre></div><blockquote class="book-hint warning"><span>The count and datatype are <strong>not used</strong> when matching up sends and
receives, it is only the source/destination pair and the tag.</span></blockquote><h3 id=message-ordering>Message ordering
<a class=anchor href=#message-ordering>#</a></h3><p>To decide on the order in which messages are processed, MPI has a rule
that messages with the same source and tag do not &ldquo;overtake&rdquo;. So if I
have</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>comm</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>COMM_WORLD</span>
<span style=color:#00a8c8>if</span> <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>:</span>
    <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Send</span><span style=color:#111>([</span><span style=color:#111>vala</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>DOUBLE</span><span style=color:#111>],</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span>
    <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Send</span><span style=color:#111>([</span><span style=color:#111>valb</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>DOUBLE</span><span style=color:#111>],</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span>
<span style=color:#00a8c8>elif</span> <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span><span style=color:#111>:</span>
    <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Recv</span><span style=color:#111>([</span><span style=color:#111>a</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>DOUBLE</span><span style=color:#111>],</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span>
    <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Recv</span><span style=color:#111>([</span><span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#ae81ff>1</span><span style=color:#111>:],</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>DOUBLE</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>)</span>
</code></pre></div><p>Then on rank 1, <code>a[0]</code> will always contain <code>vala</code> and <code>a[1]</code> will
always contain <code>valb</code>.</p><p>Let&rsquo;s look at an example. Suppose we have two processes, and we want
to send a message from rank 0 to rank 1.</p><div class=book-include><div class=book-include-heading><tt>parallel/snippets/send-message.py</tt></div><div class=book-include-download><a href=https://teaching.wence.uk/comp4187/code/parallel/snippets/send-message.py>Download</a></div><div class=book-include-content><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#f92672>import</span> <span style=color:#111>numpy</span>
<span style=color:#f92672>from</span> <span style=color:#111>mpi4py</span> <span style=color:#f92672>import</span> <span style=color:#111>MPI</span>

<span style=color:#111>comm</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>COMM_WORLD</span>

<span style=color:#111>rank</span> <span style=color:#f92672>=</span> <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>rank</span>
<span style=color:#111>size</span> <span style=color:#f92672>=</span> <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>size</span>

<span style=color:#111>value</span> <span style=color:#f92672>=</span> <span style=color:#111>numpy</span><span style=color:#f92672>.</span><span style=color:#111>empty</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>dtype</span><span style=color:#f92672>=</span><span style=color:#111>numpy</span><span style=color:#f92672>.</span><span style=color:#111>float64</span><span style=color:#111>)</span>

<span style=color:#00a8c8>if</span> <span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>:</span>
    <span style=color:#111>value</span><span style=color:#111>[:]</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>

<span style=color:#00a8c8>if</span> <span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>:</span>
    <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Ssend</span><span style=color:#111>([</span><span style=color:#111>value</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>DOUBLE</span><span style=color:#111>],</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0</span><span style=color:#111>)</span>
<span style=color:#00a8c8>elif</span> <span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span><span style=color:#111>:</span>
    <span style=color:#00a8c8>print</span><span style=color:#111>(</span><span style=color:#111>f</span><span style=color:#d88200>&#34;[{rank}]: before receiving, my value is {value}&#34;</span><span style=color:#111>,</span> <span style=color:#111>flush</span><span style=color:#f92672>=</span><span style=color:#111>True</span><span style=color:#111>)</span>
    <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Recv</span><span style=color:#111>([</span><span style=color:#111>value</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>DOUBLE</span><span style=color:#111>],</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0</span><span style=color:#111>)</span>

<span style=color:#00a8c8>print</span><span style=color:#111>(</span><span style=color:#111>f</span><span style=color:#d88200>&#34;[{rank}]: my value is {value}&#34;</span><span style=color:#111>,</span> <span style=color:#111>flush</span><span style=color:#f92672>=</span><span style=color:#111>True</span><span style=color:#111>)</span>
</code></pre></div></div></div><blockquote class=exercise><h3>Exercise</h3><span>The code above sends a message from rank 0 to rank 1. Modify it so
that it sends the message from rank 0 to ranks $[1..N]$ when run on
$N$ processes.</span></blockquote><h2 id=when-are-sends-receives-complete>When are sends (receives) complete?
<a class=anchor href=#when-are-sends-receives-complete>#</a></h2><p>Let us think about how MPI might implement sending a message over a
network. One option is that MPI copies the user data to be sent into a
buffer, sends it over the network into another buffer, and then copies
it out into the user-level receive buffer. This is shown in the figure
below.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/comp4187/images/manual/mpi-send-recv-with-buffers.svg alt="Send-receive pair with MPI-provided buffers."><figcaption><p>Send-receive pair with MPI-provided buffers.</p></figcaption></figure><p>To avoid this copy, we would like to directly send through the network</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/comp4187/images/manual/mpi-send-recv-no-buffer.svg alt="Send-receive with no buffers."><figcaption><p>Send-receive with no buffers.</p></figcaption></figure><p>For this to be possible, the send has to wait for the receive to be
available. MPI provides us with sending modes that support both of
these mechanisms.</p><h2 id=different-types-of-send-calls>Different types of send calls
<a class=anchor href=#different-types-of-send-calls>#</a></h2><blockquote class="book-hint info"><span>To see the signatures of these various mpi4py functions, use the
inbuilt help and docstrings in Python. <code>help(function_name)</code> or (in
IPython or a jupyter notebook) <code>?function_name</code>.</span></blockquote><h3 id=synchronous-send-mpi_ssend>Synchronous send: <code>MPI_Ssend</code>
<a class=anchor href=#synchronous-send-mpi_ssend>#</a></h3><p>This send mode covers the case with no buffers. The program will wait
inside the <a href=https://rookiehpc.com/mpi/docs/mpi_ssend.php><code>MPI_Ssend</code></a>
call until the matching receive is ready. The figure below shows a
timeline on two processes.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/comp4187/images/manual/mpi-ssend-cartoon.svg alt="Sketch of synchronous send between two processes."><figcaption><p>Sketch of synchronous send between two processes.</p></figcaption></figure><h3 id=buffered-send-mpi_bsend>Buffered send <code>MPI_Bsend</code>
<a class=anchor href=#buffered-send-mpi_bsend>#</a></h3><p>This send mode allows the user to provide a buffer for MPI to copy
into. The call to
<a href=https://rookiehpc.com/mpi/docs/mpi_bsend.php><code>MPI_Bsend</code></a> will
return as soon as the data are copied into the buffer. If the buffer
is too small, an error occurs.</p><figure style=width:50%><img class=scaled src=https://teaching.wence.uk/comp4187/images/manual/mpi-bsend-cartoon.svg alt="Sketch of buffered send between two processes."><figcaption><p>Sketch of buffered send between two processes.</p></figcaption></figure><blockquote class="book-hint info"><span><h4 id=points-to-note>Points to note</h4><p>The receive <code>MPI_Recv</code> is always synchronous: it waits until the
buffer is filled up with the complete received message.</p><p>In the <code>Bsend</code> case, it the receive is issued on process 1 before
process 0 starts the send, then process 1 waits in the <code>MPI_Recv</code>
call.</p></span></blockquote><h3 id=i-dont-want-to-decide-mpi_send>I don&rsquo;t want to decide: <code>MPI_Send</code>
<a class=anchor href=#i-dont-want-to-decide-mpi_send>#</a></h3><p>Managing send buffers by hand for <code>Bsend</code> is somewhat tedious, so MPI
provides a get-out option:
<a href=https://rookiehpc.com/mpi/docs/mpi_send.php><code>MPI_Send</code></a>.</p><p>In <code>MPI_Send</code>, the buffer space is provided by the MPI implementation.
If enough buffer space is available for the message, it is used (so
the send behaves like <code>Bsend</code> and returns as soon as the copy is
complete). If the buffer is full, then <code>MPI_Send</code> turns into
<code>MPI_Ssend</code>.</p><blockquote class="book-hint warning"><span>You can&rsquo;t rely on any particular size of buffer from the MPI
implementation, so you should really treat <code>MPI_Send</code> like <code>MPI_Ssend</code>.</span></blockquote><blockquote class="book-hint info"><span><h4 id=recommendation>Recommendation</h4><p><code>MPI_Bsend</code> is really an optimisation that you should apply once you
really want to squeeze the last little bit out of your implementation.</p><p>Therefore, I would only worry about <code>MPI_Send</code> and <code>MPI_Ssend</code>.
<code>MPI_Ssend</code> is <em>less forgiving</em> of incorrect code, so I recommend
<code>MPI_Ssend</code> to catch any deadlock errors.</p></span></blockquote><h3 id=a-concrete-example>A concrete example
<a class=anchor href=#a-concrete-example>#</a></h3><p>Let us look at the difference in behaviour between <code>MPI_Ssend</code> and
<code>MPI_Send</code> to observe how <code>MPI_Send</code> can hide deadlocks in some
circumstances.</p><p>Remember that <code>MPI_Send</code> returns immediately if there is enough buffer
space available, but turns into <code>MPI_Ssend</code> when the buffer space runs
out.</p><p>Here is a short snippet that illustrates the kind of problematic code.
Rank 0 will send a message to rank 1, and then receive a message from
rank 1. At the same time, rank 1 first sends a message to rank 0.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#00a8c8>if</span> <span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>:</span>
    <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Send</span><span style=color:#111>([</span><span style=color:#111>send</span><span style=color:#111>,</span> <span style=color:#111>n</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>INT</span><span style=color:#111>],</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0</span><span style=color:#111>)</span>
    <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Recv</span><span style=color:#111>([</span><span style=color:#111>recv</span><span style=color:#111>,</span> <span style=color:#111>n</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>INT</span><span style=color:#111>],</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0</span><span style=color:#111>)</span>
<span style=color:#00a8c8>elif</span> <span style=color:#111>rank</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span><span style=color:#111>:</span>
    <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Send</span><span style=color:#111>([</span><span style=color:#111>send</span><span style=color:#111>,</span> <span style=color:#111>n</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>INT</span><span style=color:#111>],</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0</span><span style=color:#111>)</span>
    <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Recv</span><span style=color:#111>([</span><span style=color:#111>recv</span><span style=color:#111>,</span> <span style=color:#111>n</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>INT</span><span style=color:#111>],</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0</span><span style=color:#111>)</span>
</code></pre></div><blockquote class=exercise><h3>Exercise</h3><span><p>The code <a href=https://teaching.wence.uk/comp4187/code/parallel/snippets/ptp-deadlock.py><code>parallel/snippets/ptp-deadlock.py</code></a> implements this message passing
deadlock.</p><p>It takes one argument, which is the size of message to send.</p><details><summary>Hint</summary><div class=markdown-inner>Don&rsquo;t forget to load the <a href=https://teaching.wence.uk/comp4187/exercises/parallel/hello/#mpi>relevant MPI module</a>.</div></details><p>Run it on two processes.</p><p>How big can you make this message before you observe a
deadlock?</p><details><summary>Cancelling the process</summary><div class=markdown-inner><p>If you launched the run interactively, type <code>Control-c</code> to quite the
hanging process.</p><p>If you used the batch system you can use <code>scancel</code> followed by the ID
of the job to cancel the job (or set a short timeout in your slurm script).</p></div></details><p>Try changing the <code>comm.Send</code> calls to <code>comm.Ssend</code>, is there now any
value of the buffer size that completes successfully?</p></span></blockquote><h2 id=avoiding-deadlocks>Avoiding deadlocks
<a class=anchor href=#avoiding-deadlocks>#</a></h2><h3 id=pairwise-communication-mpi_sendrecv>Pairwise communication: <code>MPI_Sendrecv</code>
<a class=anchor href=#pairwise-communication-mpi_sendrecv>#</a></h3><p>For simple pairwise communication, like our example of exchanging
messages, MPI offers an function that does the equivalent of executing
a send and a receive <em>simultaneously</em> (avoiding the deadlock problem
of sends coming before receives).</p><p><a href=https://rookiehpc.com/mpi/docs/mpi_sendrecv.php><code>MPI_Sendrecv</code></a>
pairs up a send and a receive in one call.</p><blockquote class=exercise><h3>Exercise</h3><span>Rewrite the code of <a href=https://teaching.wence.uk/comp4187/code/parallel/snippets/send-message.py><code>parallel/snippets/send-message.py</code></a> to use <code>MPI_Sendrecv</code>.</span></blockquote><h3 id=non-blocking-communication>Non-blocking communication
<a class=anchor href=#non-blocking-communication>#</a></h3><p>The pairwise send-receive is useful. but not general enough to cover
all point-to-point communication patterns we might encounter. MPI
therefore offers &ldquo;non-blocking&rdquo; communication modes that return
immediately and allow us to later test if the message has been
sent/received.</p><p>This page is already long enough, so they&rsquo;re described in detail
<a href=https://teaching.wence.uk/comp4187/lectures/mpi/point-to-point-nb/>separately</a>.</p><h2 id=summary>Summary
<a class=anchor href=#summary>#</a></h2><p>MPI has flexible point-to-point messaging. The message contents are
described by a pointer to a buffer (to send from/receive into) along
with a count and datatype.</p><p>The source or destination of a message is specified by providing the
communicator and a rank.</p><p>Messages can be distinguished by tags. Often don&rsquo;t need them for
simple processes, but can be used in advanced usage, or to make sure
that messages don&rsquo;t accidentally match.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>After <code>#include &lt;stdint.h></code> <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/comp4187/commit/9271858aa631161252031b9cd38d4da1c5c802a0 title="Last modified by Lawrence Mitchell | February 14, 2024" target=_blank rel=noopener><img src=/comp4187/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 14, 2024</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/comp4187/edit/main/site/content/lectures/mpi/point-to-point.md target=_blank rel=noopener><img src=/comp4187/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence@wence.uk>Lawrence Mitchell</a>, <a href=https://annereinarz.github.io>Anne Reinarz</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/comp4187/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#pairwise-message-exchange>Pairwise message exchange</a><ul><li><a href=#describing-the-data>Describing the data</a></li><li><a href=#identification-of-processes-and-distinguishing-messages>Identification of processes and distinguishing messages</a><ul><li><a href=#message-ordering>Message ordering</a></li></ul></li><li><a href=#when-are-sends-receives-complete>When are sends (receives) complete?</a></li><li><a href=#different-types-of-send-calls>Different types of send calls</a><ul><li><a href=#synchronous-send-mpi_ssend>Synchronous send: <code>MPI_Ssend</code></a></li><li><a href=#buffered-send-mpi_bsend>Buffered send <code>MPI_Bsend</code></a></li><li><a href=#i-dont-want-to-decide-mpi_send>I don&rsquo;t want to decide: <code>MPI_Send</code></a></li><li><a href=#a-concrete-example>A concrete example</a></li></ul></li><li><a href=#avoiding-deadlocks>Avoiding deadlocks</a><ul><li><a href=#pairwise-communication-mpi_sendrecv>Pairwise communication: <code>MPI_Sendrecv</code></a></li><li><a href=#non-blocking-communication>Non-blocking communication</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></li></ul></nav></aside></main></body></html>