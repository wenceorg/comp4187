<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Non-blocking messages #  As well as the blocking point to point messaging we saw last time, MPI also offers non-blocking versions.
These functions all return immediately, and provide a &ldquo;request&rdquo; object that we can then either wait for completion with or inspect to check if the message has been sent/received.
The function signatures for MPI_Isend and MPI_Irecv are:
int MPI_Isend(const void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); int MPI_Irecv(void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); The mpi4py versions are:"><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Non-blocking point-to-point messaging"><meta property="og:description" content="Non-blocking messages #  As well as the blocking point to point messaging we saw last time, MPI also offers non-blocking versions.
These functions all return immediately, and provide a &ldquo;request&rdquo; object that we can then either wait for completion with or inspect to check if the message has been sent/received.
The function signatures for MPI_Isend and MPI_Irecv are:
int MPI_Isend(const void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); int MPI_Irecv(void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); The mpi4py versions are:"><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/comp4187/lectures/mpi/point-to-point-nb/"><meta property="article:modified_time" content="2024-02-14T14:35:09+00:00"><title>Non-blocking point-to-point messaging | COMP4187 – Parallel Scientific Computing II</title><link rel=manifest href=/comp4187/manifest.json><link rel=icon href=/comp4187/favicon.png type=image/x-icon><link rel=stylesheet href=/comp4187/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/comp4187/logo.svg alt=Logo><h2><a href=/comp4187>COMP4187 – Parallel Scientific Computing II</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/comp4187/setup/jupyter/>Jupyter</a></li><li><a href=/comp4187/setup/mpi/>MPI</a></li></ul></li><li><span>Exercises</span><ul><li><span>Parallel</span><ul><li><a href=/comp4187/exercises/parallel/hello/>Parallel Hello World</a></li><li><a href=/comp4187/exercises/parallel/pi/>Calculating π</a></li><li><a href=/comp4187/exercises/parallel/domain-decomp-simple/>1-D domain decomposition</a></li><li><a href=/comp4187/exercises/parallel/pingpong/>Ping-pong latency</a></li></ul></li><li><a href=/comp4187/exercises/coarse-grid/>Coarse Grid Operator</a></li><li><a href=/comp4187/exercises/finite-differences/>Finite Differences</a></li><li><a href=/comp4187/exercises/two-grid/>Two-Grid Iteration</a></li><li><a href=/comp4187/exercises/norms/>Norms</a></li></ul></li><li><span>Notes</span><ul><li><span>Lectures: Numerics</span><ul><li><a href=/comp4187/lectures/numerics/lecture1/>Lecture 1: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture2/>Lecture 2: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture3/>Lecture 3: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture4/>Lecture 4: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture5/>Lecture 5: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture6/>Lecture 6: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture7/>Lecture 7: Linear Solvers</a></li><li><a href=/comp4187/lectures/numerics/lecture8/>Lecture 8: Linear Solvers</a></li><li><a href=/comp4187/lectures/numerics/lecture9/>Lecture 9: Multigrid</a></li></ul></li><li><a href=/comp4187/lectures/mpi/>MPI</a><ul><li><a href=/comp4187/lectures/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/comp4187/lectures/mpi/point-to-point-nb/ class=active>Non-blocking point-to-point messaging</a></li><li><a href=/comp4187/lectures/mpi/collectives/>Collectives</a></li><li><a href=/comp4187/lectures/mpi/advanced/>Advanced topics</a></li><li><a href=/comp4187/lectures/mpi/petsc4py/>PETSc and petsc4py</a></li><li><a href=/comp4187/lectures/mpi/live-notes/>Term 2: live lecture notes</a></li></ul></li></ul></li><li><a href=/comp4187/coursework/>Coursework 1: Euler-Bernoulli Beam Theory</a></li><li><a href=/comp4187/coursework2/>Coursework 2: multigrid solvers</a></li><li><a href=/comp4187/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/comp4187/past-editions/2020-21/term1/>Term 1: numerics</a></li><li><a href=/comp4187/past-editions/2020-21/term2/>Term 2: parallel computing</a></li><li><a href=/comp4187/past-editions/2020-21/coursework/>Coursework: a 3D multigrid solver</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/comp4187/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Non-blocking point-to-point messaging</strong>
<label for=toc-control><img src=/comp4187/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#non-blocking-messages>Non-blocking messages</a><ul><li><a href=#why-would-you-do-this>Why would you do this?</a></li><li><a href=#waiting-for-multiple-messages>Waiting for multiple messages</a></li><li><a href=#wildcards>Wildcard matching</a></li><li><a href=#summary>Summary</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=non-blocking-messages>Non-blocking messages
<a class=anchor href=#non-blocking-messages>#</a></h1><p>As well as the blocking point to point messaging we saw <a href=https://teaching.wence.uk/comp4187/lectures/mpi/point-to-point/>last
time</a>, MPI also offers <em>non-blocking</em>
versions.</p><p>These functions all return immediately, and provide a &ldquo;request&rdquo; object
that we can then either wait for completion with or inspect to check
if the message has been sent/received.</p><p>The function signatures for
<a href=https://rookiehpc.com/mpi/docs/mpi_isend.php><code>MPI_Isend</code></a> and
<a href=https://rookiehpc.com/mpi/docs/mpi_irecv.php><code>MPI_Irecv</code></a> are:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Isend</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>dtype</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#111>MPI_Request</span> <span style=color:#f92672>*</span><span style=color:#111>request</span><span style=color:#111>);</span>
<span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Irecv</span><span style=color:#111>(</span><span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>dtype</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>,</span> <span style=color:#111>MPI_Request</span> <span style=color:#f92672>*</span><span style=color:#111>request</span><span style=color:#111>);</span>
</code></pre></div><p>The mpi4py versions are:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>request</span> <span style=color:#f92672>=</span> <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Isend</span><span style=color:#111>([</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>datatype</span><span style=color:#111>],</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#111>)</span>
<span style=color:#111>request</span> <span style=color:#f92672>=</span> <span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Irecv</span><span style=color:#111>([</span><span style=color:#111>buffer</span><span style=color:#111>,</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>datatype</span><span style=color:#111>],</span> <span style=color:#111>src</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#111>)</span>
</code></pre></div><p>So both sends and receives now <em>return</em> a request object.</p><blockquote class="book-hint warning"><span><p>With the <a href=https://teaching.wence.uk/comp4187/lectures/mpi/point-to-point/>blocking versions</a>
(<code>MPI_Send</code>, <code>MPI_Ssend</code>, <code>MPI_Bsend</code>), the buffer argument is safe to
reuse <em>as soon as the function returns</em>. Equally, as soon as
<code>MPI_Recv</code> returns, we know the message has been received and we can
inspect the contents.</p><p><strong>This is not the case</strong> for non-blocking calls.</p><p>We are not allowed to reuse the buffer (or rely on its contents being
ready) until we have &ldquo;waited&rdquo; on the <code>request</code> handle.</p><p>See below for details on how to do this.</p></span></blockquote><p>If we have a request, we can check whether the message it corresponds
to has been completed with <a href=https://rookiehpc.com/mpi/docs/mpi_test.php><code>MPI_Test</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>request</span><span style=color:#f92672>.</span><span style=color:#111>Test</span><span style=color:#111>()</span>
<span style=color:#f92672>=&gt;</span> <span style=color:#111>True</span>  <span style=color:#75715e># if the request is completed (message is sent/received)</span>
<span style=color:#f92672>=&gt;</span> <span style=color:#111>False</span> <span style=color:#75715e># otherwise.</span>
</code></pre></div><p>The return value will be true if the provided request has been
completed, and false otherwise.</p><p>If instead we want to wait for completion, we can use
<a href=https://rookiehpc.com/mpi/docs/mpi_wait.php><code>MPI_Wait</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>request</span><span style=color:#f92672>.</span><span style=color:#111>Wait</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>Status</span> <span style=color:#111>status</span><span style=color:#f92672>=</span><span style=color:#111>None</span><span style=color:#111>)</span>
</code></pre></div><p>Which waits until the message corresponding to <code>request</code> has been
completed.</p><p>Both of these calls can <em>complete</em> the message exchange. If <code>MPI_Test</code>
returns <code>True</code>, the message has been sent/received
and the user-provided send/receive buffer is safe to be used again.</p><p>Here&rsquo;s a picture of a non-blocking <code>MPI_Issend</code> matching with a
blocking <code>MPI_Recv</code>. Note how the data transfer does not start
(because this is a synchronous send) until the matching receive has
been posted (set up). So the first <code>MPI_Test</code> returns false. The
<code>MPI_Wait</code> will return immediately because the message has now been
transferred.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/comp4187/images/manual/mpi-issend-cartoon.svg alt="A non-blocking synchronous send returns immediately, and the data transfer begins as soon as the matching receive appears."><figcaption><p>A non-blocking synchronous send returns immediately, and the data transfer begins as soon as the matching receive appears.</p></figcaption></figure><h2 id=why-would-you-do-this>Why would you do this?
<a class=anchor href=#why-would-you-do-this>#</a></h2><p>Non-blocking messages allow us to separate &ldquo;posting&rdquo; messages
from when we check if they are completed. One reason to do this is
that MPI libraries often have optimisations to complete sends quickly
if the matching receive already exists.</p><p>If I am receiving messages from 10 different processes, if I use
a blocking <code>MPI_Recv</code>, then there is only ever one receive ready at
any one time. Conversely if I use <code>MPI_Irecv</code>, then all receives will
be ready, and the MPI library can complete them as the matching send
arrives.</p><p>It also allows us to simplify programs that exchange many messages if
we&rsquo;re trying to avoid deadlocks. We can just post all sends/receives
at once and then wait, rather than having to arrange that we have a
single send/receive ready at the right time.</p><p>Finally, non-blocking communication allows us to (in theory) <em>overlap</em>
communication with computation. This can help to improve scaling
performance in some cases.</p><p>As you probably saw when doing the <a href=https://teaching.wence.uk/comp4187/exercises/parallel/pingpong/>ping-pong</a> exercise, all MPI messages have a non-zero
latency. That means that no matter how small it is, it takes some
time for a message to cross the network. If we use blocking messages,
the best case total time for our simulation is going to be</p><p>$$
T_{\text{compute}} + T_{\text{communicate}}
$$</p><p>Many scientific computing simulations have compute and communication
parts that can <em>overlap</em>. For example, when domain decomposing a mesh
for a parallel PDE solver, most of the computation can be done
without communicating with our neighbours: we only need information
when we&rsquo;re near the edge of our local domain. We can therefore often
split the simulation into phases:</p><ol><li>Send data to neighbours</li><li>Compute on local data that doesn&rsquo;t depend on neighbours</li><li>Receive data from neighbours</li><li>Compute on remaining local data</li></ol><p>If we use non-blocking messages, we can sometimes hide the latency in
steps (1) and (3), so that the total simulation time is now</p><p>$$
\max(T_{\text{compute}}, T_{\text{communicate}}) &lt; T_{\text{compute}} + T_{\text{communicate}}
$$</p><p>The implementation of halo exchanges in the Grid data structure will
use this facility.</p><h2 id=waiting-for-multiple-messages>Waiting for multiple messages
<a class=anchor href=#waiting-for-multiple-messages>#</a></h2><p>The advantage of the non-blocking communication mode becomes more
apparent when we look at waiting or testing for completion of multiple
messages simultaneously.</p><p>A typical pseudo-code with non-blocking communication might look
something like this</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>requests</span> <span style=color:#f92672>=</span> <span style=color:#111>[]</span>
<span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>nrecv</span><span style=color:#111>):</span>
    <span style=color:#111>requests</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Irecv</span><span style=color:#111>(</span><span style=color:#f92672>...</span><span style=color:#111>))</span>

<span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>nsend</span><span style=color:#111>):</span>
    <span style=color:#111>requests</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Isend</span><span style=color:#111>(</span><span style=color:#f92672>...</span><span style=color:#111>))</span>

<span style=color:#75715e># Some work that doesn&#39;t depend on the messages</span>
<span style=color:#f92672>...</span>

</code></pre></div><p>Having done the work that doesn&rsquo;t depend on messages, we now need to
wait for message completion.</p><p>Perhaps we need all the messages to complete, in which case we can use
<a href=https://rookiehpc.com/mpi/docs/mpi_waitall.php><code>MPI_Waitall</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#111>MPI</span><span style=color:#111>.</span><span style=color:#111>Request</span><span style=color:#111>.</span><span style=color:#111>Waitall</span><span style=color:#111>(</span><span style=color:#111>requests</span><span style=color:#111>)</span>
</code></pre></div><p>This approach is preferred over a loop calling <code>MPI_Wait</code> on each
request, since the MPI implementation is free to process the arriving
messages in any order when we call <code>MPI_Waitall</code> which might speed
things up.</p><p>Perhaps we just want a message to have arrived, in which case we can
use <a href=https://rookiehpc.com/mpi/docs/mpi_waitany.php><code>MPI_Waitany</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>which</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>Request</span><span style=color:#f92672>.</span><span style=color:#111>Waitany</span><span style=color:#111>(</span><span style=color:#111>requests</span><span style=color:#111>)</span>
</code></pre></div><p>Now the <code>which</code> variable tells us which of the requests completed.</p><p>Finally, suppose we want to wait until <em>at least one</em> message has
completed, we can use
<a href=https://rookiehpc.com/mpi/docs/mpi_waitsome.php><code>MPI_Waitsome</code></a></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>indices</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>Request</span><span style=color:#f92672>.</span><span style=color:#111>Waitsome</span><span style=color:#111>(</span><span style=color:#111>requests</span><span style=color:#111>)</span>
<span style=color:#75715e># Now len(indices) tells us how many requests are completed,</span>
<span style=color:#75715e># and indices[0..nfinished-1] tells us which requests they are</span>
</code></pre></div><p>There are also matching
<a href=https://rookiehpc.com/mpi/docs/mpi_testall.php><code>MPI_Testall</code></a>,
<a href=https://rookiehpc.com/mpi/docs/mpi_testany.php><code>MPI_Testany</code></a>, and
<a href=https://rookiehpc.com/mpi/docs/mpi_testsome.php><code>MPI_Testsome</code></a>
calls which don&rsquo;t block for completion of the messages.</p><p>A high quality MPI implementation will provide optimised code for
these routines that is more efficient than a loop with
<code>MPI_Test</code>/<code>MPI_Wait</code> pairs.</p><blockquote class=exercise><h3>Exercise</h3><span><h3 id=gathering-data-from-every-process>Gathering data from every process</h3><p>Write an MPI code in which rank-0 gathers a message from every process
and places it in an array at a position corresponding to the rank of
the sender.</p><p>So if running with $P$ processes, rank-0 should allocate an array with
space for $P$ entries, and after collecting the messages.</p><p>Compare the performance of two versions.</p><ol><li>rank-0 uses a blocking <code>MPI_Recv</code> for all receives</li><li>rank-0 uses non-blocking <code>MPI_Irecv</code> followed by <code>MPI_Waitall</code>.</li></ol><p>Which performs better as a function of the total number of messages, $P$?</p></span></blockquote><h2 id=wildcards>Wildcard matching
<a class=anchor href=#wildcards>#</a></h2><p>So far, we&rsquo;ve always specified specific <code>source</code> and <code>tag</code> arguments in
the arguments to <code>MPI_Recv</code> and <code>MPI_Irecv</code>. MPI also provides us with
the option to say &ldquo;receive a message, I don&rsquo;t care who its from, or
what the tag is&rdquo;.</p><p>We do that by providing <code>MPI_ANY_SOURCE</code> and/or <code>MPI_ANY_TAG</code> as the
source and tag arguments respectively.</p><p>We can subsequently, find out where we got the message from, and what
its tag was, by inspecting the <code>status</code> object that <code>MPI_Recv</code>
returns.</p><p>Up to now, we&rsquo;ve just said <code>MPI_STATUS_IGNORE</code>, but we can also do</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-py data-lang=py><span style=color:#111>status</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>Status</span><span style=color:#111>()</span>
<span style=color:#111>comm</span><span style=color:#f92672>.</span><span style=color:#111>Recv</span><span style=color:#111>(</span><span style=color:#f92672>...</span><span style=color:#111>,</span> <span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>ANY_SOURCE</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#f92672>=</span><span style=color:#111>MPI</span><span style=color:#f92672>.</span><span style=color:#111>ANY_TAG</span><span style=color:#111>,</span> <span style=color:#111>status</span><span style=color:#f92672>=</span><span style=color:#111>status</span><span style=color:#111>)</span>

<span style=color:#111>status</span><span style=color:#f92672>.</span><span style=color:#111>source</span>                   <span style=color:#75715e># The source rank</span>
<span style=color:#111>status</span><span style=color:#f92672>.</span><span style=color:#111>tag</span>                      <span style=color:#75715e># The tag</span>
</code></pre></div><p>There actually aren&rsquo;t that many reasons you would use wildcards in
receives. They can be useful when implementing <a href="http://htor.inf.ethz.ch/publications/index.php?pub=99">dynamic sparse data
exchange</a>.</p><blockquote class="book-hint info"><span>Typically, the implementation of &ldquo;wildcard&rdquo; matching is less efficient
than message matching with given source and tag arguments.</span></blockquote><h2 id=summary>Summary
<a class=anchor href=#summary>#</a></h2><p>As well as providing blocking send/receive options, MPI provides
non-blocking versions.</p><p>These allow us to potentially improve performance of message exchange,
and simplify writing algorithms that need to match many pairs of
messages, without thinking as hard about potential deadlocks.</p><p>The critical thing to recall is that <strong>we are not allowed</strong> to look at
the buffers we pass into non-blocking sends/receives until after
calling a blocking <code>MPI_Wait</code>-like call, or a non-blocking
<code>MPI_Test</code>-like call has returned true.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/comp4187/commit/9271858aa631161252031b9cd38d4da1c5c802a0 title="Last modified by Lawrence Mitchell | February 14, 2024" target=_blank rel=noopener><img src=/comp4187/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 14, 2024</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/comp4187/edit/main/site/content/lectures/mpi/point-to-point-nb.md target=_blank rel=noopener><img src=/comp4187/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence@wence.uk>Lawrence Mitchell</a>, <a href=https://annereinarz.github.io>Anne Reinarz</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/comp4187/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#non-blocking-messages>Non-blocking messages</a><ul><li><a href=#why-would-you-do-this>Why would you do this?</a></li><li><a href=#waiting-for-multiple-messages>Waiting for multiple messages</a></li><li><a href=#wildcards>Wildcard matching</a></li><li><a href=#summary>Summary</a></li></ul></li></ul></nav></aside></main></body></html>