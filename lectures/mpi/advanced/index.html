<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Some pointers to more advanced features of MPI #  Communicator manipulation #  We saw that we can distinguish point-to-point messages by providing different tags, but that there was no such facility for collective operations. Moreover, a collective operation (by definition) involves all the processes in a communicator.
This raises two questions:
 How can we have multiple collective operations without them interfering with each other; What if we want a collective operation, but using only a subset of the processes (e."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Advanced topics"><meta property="og:description" content="Some pointers to more advanced features of MPI #  Communicator manipulation #  We saw that we can distinguish point-to-point messages by providing different tags, but that there was no such facility for collective operations. Moreover, a collective operation (by definition) involves all the processes in a communicator.
This raises two questions:
 How can we have multiple collective operations without them interfering with each other; What if we want a collective operation, but using only a subset of the processes (e."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/comp4187/lectures/mpi/advanced/"><meta property="article:modified_time" content="2024-02-14T14:35:09+00:00"><title>Advanced topics | COMP4187 – Parallel Scientific Computing II</title><link rel=manifest href=/comp4187/manifest.json><link rel=icon href=/comp4187/favicon.png type=image/x-icon><link rel=stylesheet href=/comp4187/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/comp4187/logo.svg alt=Logo><h2><a href=/comp4187>COMP4187 – Parallel Scientific Computing II</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/comp4187/setup/jupyter/>Jupyter</a></li><li><a href=/comp4187/setup/mpi/>MPI</a></li></ul></li><li><span>Exercises</span><ul><li><span>Parallel</span><ul><li><a href=/comp4187/exercises/parallel/hello/>Parallel Hello World</a></li><li><a href=/comp4187/exercises/parallel/pi/>Calculating π</a></li><li><a href=/comp4187/exercises/parallel/domain-decomp-simple/>1-D domain decomposition</a></li><li><a href=/comp4187/exercises/parallel/pingpong/>Ping-pong latency</a></li></ul></li><li><a href=/comp4187/exercises/coarse-grid/>Coarse Grid Operator</a></li><li><a href=/comp4187/exercises/finite-differences/>Finite Differences</a></li><li><a href=/comp4187/exercises/two-grid/>Two-Grid Iteration</a></li><li><a href=/comp4187/exercises/norms/>Norms</a></li></ul></li><li><span>Notes</span><ul><li><span>Lectures: Numerics</span><ul><li><a href=/comp4187/lectures/numerics/lecture1/>Lecture 1: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture2/>Lecture 2: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture3/>Lecture 3: Time-stepping</a></li><li><a href=/comp4187/lectures/numerics/lecture4/>Lecture 4: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture5/>Lecture 5: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture6/>Lecture 6: Finite Differences</a></li><li><a href=/comp4187/lectures/numerics/lecture7/>Lecture 7: Linear Solvers</a></li><li><a href=/comp4187/lectures/numerics/lecture8/>Lecture 8: Linear Solvers</a></li><li><a href=/comp4187/lectures/numerics/lecture9/>Lecture 9: Multigrid</a></li></ul></li><li><a href=/comp4187/lectures/mpi/>MPI</a><ul><li><a href=/comp4187/lectures/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/comp4187/lectures/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/comp4187/lectures/mpi/collectives/>Collectives</a></li><li><a href=/comp4187/lectures/mpi/advanced/ class=active>Advanced topics</a></li><li><a href=/comp4187/lectures/mpi/petsc4py/>PETSc and petsc4py</a></li><li><a href=/comp4187/lectures/mpi/live-notes/>Term 2: live lecture notes</a></li></ul></li></ul></li><li><a href=/comp4187/coursework/>Coursework 1: Euler-Bernoulli Beam Theory</a></li><li><a href=/comp4187/coursework2/>Coursework 2: multigrid solvers</a></li><li><a href=/comp4187/acknowledgements/>Acknowledgements</a></li><li><span>Past editions</span><ul><li><span>2020/21</span><ul><li><a href=/comp4187/past-editions/2020-21/term1/>Term 1: numerics</a></li><li><a href=/comp4187/past-editions/2020-21/term2/>Term 2: parallel computing</a></li><li><a href=/comp4187/past-editions/2020-21/coursework/>Coursework: a 3D multigrid solver</a></li></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/comp4187/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Advanced topics</strong>
<label for=toc-control><img src=/comp4187/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#some-pointers-to-more-advanced-features-of-mpi>Some pointers to more advanced features of MPI</a><ul><li><a href=#communicators>Communicator manipulation</a><ul><li><a href=#duplicating-communicators>Duplicating communicators</a></li><li><a href=#splitting-communicators-into-subgroups>Splitting communicators into subgroups</a></li></ul></li><li><a href=#further-features-and-details>Further features and details</a><ul><li><a href=#file-io>File IO</a></li><li><a href=#profiling-interfacehttpswwwmpi-forumorgdocsmpi-31mpi31-reportnode357htmnode357><a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node357.htm#Node357>Profiling interface</a></a></li><li><a href=#one-sided-messaging-and-remote-memory-access>One-sided messaging and Remote Memory Access</a></li></ul></li><li><a href=#language-bindings-and-libraries>Language bindings and libraries</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=some-pointers-to-more-advanced-features-of-mpi>Some pointers to more advanced features of MPI
<a class=anchor href=#some-pointers-to-more-advanced-features-of-mpi>#</a></h1><h2 id=communicators>Communicator manipulation
<a class=anchor href=#communicators>#</a></h2><p>We saw that we can distinguish point-to-point messages by providing
different tags, but that there was no such facility for collective
operations. Moreover, a collective operation (by definition) involves
all the processes in a communicator.</p><p>This raises two questions:</p><ol><li>How can we have multiple collective operations without them
interfering with each other;</li><li>What if we want a collective operation, but using only a subset of
the processes (e.g. only processes with an even rank)?</li></ol><p>We might worry that we&rsquo;re reduced to writing everything by hand using
point-to-point messages, but fear not, MPI has us covered.</p><h3 id=duplicating-communicators>Duplicating communicators
<a class=anchor href=#duplicating-communicators>#</a></h3><p>To address point 1, collective operations match based on the
communicator context, and MPI allows us to
<a href=https://www.mpich.org/static/docs/v3.3/www3/MPI_Comm_dup.html><em>duplicate</em></a>
communicators. This provides us with a new communicator that contains
exactly the same set of processes with the same ranks, but collectives
on one communicator won&rsquo;t interfere with those on another (and
similarly for point-to-point messages).</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Comm_dup</span><span style=color:#111>(</span><span style=color:#111>MPI_Comm</span> <span style=color:#111>incomm</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#f92672>*</span><span style=color:#111>outcomm</span><span style=color:#111>);</span>
</code></pre></div><p>This is a very useful thing to use if you are writing a library that
uses MPI. Whenever someone calls your library you do</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#111>MPI_Comm_dup</span><span style=color:#111>(</span><span style=color:#111>user_communicator</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>library_communicator</span><span style=color:#111>);</span>
</code></pre></div><p>and then always use the <code>library_communicator</code> inside your library.
Now you can <em>guarantee</em> that you will never accidentally match any
messages or collectives that the user runs on their
<code>user_communicator</code>.</p><p>When we are done, we should release the communicator we duplicated (so
as not to leak memory) by calling <code>MPI_Comm_free</code></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Comm_free</span><span style=color:#111>(</span><span style=color:#111>MPI_Comm</span> <span style=color:#f92672>*</span><span style=color:#111>comm</span><span style=color:#111>);</span>

<span style=color:#75715e>/* To release a communicator: */</span>
<span style=color:#111>MPI_Comm_free</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>library_communicator</span><span style=color:#111>);</span>
</code></pre></div><h3 id=splitting-communicators-into-subgroups>Splitting communicators into subgroups
<a class=anchor href=#splitting-communicators-into-subgroups>#</a></h3><p>This is useful if want some collective operation over a subset of all
the processes, for example we want to gather along the rows of a
distributed matrix. This can be done by calling <code>MPI_Comm_split</code></p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Comm_split</span><span style=color:#111>(</span><span style=color:#111>MPI_Comm</span> <span style=color:#111>incomm</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>colour</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>key</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#f92672>*</span><span style=color:#111>newcomm</span><span style=color:#111>);</span>
</code></pre></div><p>The <code>colour</code> decides which ranks in <code>incomm</code> end up in the same
<code>newcomm</code>. Ranks that provide the same <code>colour</code> will be in the same
group. The <code>key</code> can be used to provide an ordering of the ranks in
the new group, usually we pass the rank from the <code>incomm</code>.</p><p>For example, to create a communicator that splits into the processes
in <code>MPI_COMM_WORLD</code> into a even and odd processes we can use.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#111>rank</span><span style=color:#111>;</span>
<span style=color:#111>MPI_Comm</span> <span style=color:#111>incomm</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI_COMM_WORLD</span><span style=color:#111>;</span>
<span style=color:#111>MPI_Comm</span> <span style=color:#111>newcomm</span><span style=color:#111>;</span>
<span style=color:#111>MPI_Comm_rank</span><span style=color:#111>(</span><span style=color:#111>incomm</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>rank</span><span style=color:#111>)</span>
<span style=color:#111>MPI_Comm_split</span><span style=color:#111>(</span><span style=color:#111>incomm</span><span style=color:#111>,</span> <span style=color:#111>rank</span> <span style=color:#f92672>%</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>rank</span><span style=color:#111>,</span> <span style=color:#f92672>&amp;</span><span style=color:#111>newcomm</span><span style=color:#111>);</span>
<span style=color:#75715e>/* Do stuff with newcomm */</span>
<span style=color:#75715e>/* Release once we are done */</span>
<span style=color:#111>MPI_Comm_free</span><span style=color:#111>(</span><span style=color:#f92672>&amp;</span><span style=color:#111>newcomm</span><span style=color:#111>);</span>
</code></pre></div><p>Here&rsquo;s a picture:</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/comp4187/images/manual/comm-world-split.svg alt="MPI_Comm_split can split a communicator into smaller ones which can then proceed independently."><figcaption><p><code>MPI_Comm_split</code> can split a communicator into smaller ones which can then proceed independently.</p></figcaption></figure><p>We emphasise again that this does not produce new processes, it just
provides a communication context that does not contain all processes.</p><blockquote class=exercise><h3>Exercise</h3><span><p><a href=https://teaching.wence.uk/comp4187/code/code/mpi-snippets/split-comm.c><code>code/mpi-snippets/split-comm.c</code></a> contains a simple example.
Have a look at the code and compile and run it.</p><p>Do you understand the output?</p><p>Do you understand why there is only one <code>splitcomm</code> variable (despite
splitting the input communicator into two)?</p></span></blockquote><p>This splitting facility is useful if we only need a subset of all the
processes to participate in a collective operation. This might be
something like reducing the number of processes participating in the
<a href=https://arxiv.org/abs/1604.07163>coarse grids of a multigrid solve</a>.</p><h2 id=further-features-and-details>Further features and details
<a class=anchor href=#further-features-and-details>#</a></h2><p>In addition to what we&rsquo;ve seen, MPI provides a number of other
features that are useful for writing libraries. We won&rsquo;t cover them in
detail, but just mention some aspects.</p><h3 id=file-io>File IO
<a class=anchor href=#file-io>#</a></h3><p>MPI, via
<a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node305.htm#Node305>MPI-IO</a>,
provides a portable and high-performance way of reading and writing
files in parallel. This forms the backbone of higher-level parallel
file libraries like <a href=https://www.hdfgroup.org>HDF5</a> and
<a href=https://www.unidata.ucar.edu/software/netcdf/>NetCDF</a>.</p><h3 id=profiling-interfacehttpswwwmpi-forumorgdocsmpi-31mpi31-reportnode357htmnode357><a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node357.htm#Node357>Profiling interface</a>
<a class=anchor href=#profiling-interfacehttpswwwmpi-forumorgdocsmpi-31mpi31-reportnode357htmnode357>#</a></h3><p>All MPI functions (everything called <code>MPI_XXX</code>) are actually just
wrappers around internal &ldquo;profiling&rdquo; functions whose names start with
<code>PMPI_XXX</code>. For example, <code>MPI_Send</code> is implemented in the MPI library as</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Send</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>datatype</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>return</span> <span style=color:#111>PMPI_Send</span><span style=color:#111>(</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>datatype</span><span style=color:#111>,</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div><p>The public <code>MPI_</code> functions are exported with <a href=https://en.wikipedia.org/wiki/Weak_symbol>weak symbol
binding</a> so we can override
them. For example, suppose that we want to print a message every time
an <code>MPI_Send</code> is called, our code could do:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#75af00>MPI_Send</span><span style=color:#111>(</span><span style=color:#00a8c8>const</span> <span style=color:#00a8c8>void</span> <span style=color:#f92672>*</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>MPI_Datatype</span> <span style=color:#111>datatype</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>MPI_Comm</span> <span style=color:#111>comm</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#111>printf</span><span style=color:#111>(</span><span style=color:#d88200>&#34;Sending message to %d</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#34;</span><span style=color:#111>,</span> <span style=color:#111>dest</span><span style=color:#111>);</span>
  <span style=color:#00a8c8>return</span> <span style=color:#111>PMPI_Send</span><span style=color:#111>(</span><span style=color:#111>sendbuf</span><span style=color:#111>,</span> <span style=color:#111>count</span><span style=color:#111>,</span> <span style=color:#111>datatype</span><span style=color:#111>,</span> <span style=color:#111>dest</span><span style=color:#111>,</span> <span style=color:#111>tag</span><span style=color:#111>,</span> <span style=color:#111>comm</span><span style=color:#111>);</span>
<span style=color:#111>}</span>
</code></pre></div><p>This facility is used to write tools that can produce timelines of
message-passing in a parallel program. These include</p><ul><li><a href=https://www.mcs.anl.gov/research/projects/perfvis/software/MPE/>The MPI Parallel Environment</a></li><li><a href=https://www.vi-hps.org/projects/score-p/>Score-P</a></li><li><a href=http://www.scalasca.org>Scalasca</a></li><li><a href=http://www.cs.uoregon.edu/research/tau/home.php>Tau</a></li></ul><h3 id=one-sided-messaging-and-remote-memory-access>One-sided messaging and Remote Memory Access
<a class=anchor href=#one-sided-messaging-and-remote-memory-access>#</a></h3><p>All of the messaging we saw was <em>two-sided</em>, in that we need both a
send and a receive. MPI-2 introduced, and MPI-3 extended and improved,
support for one-sided messages and direct access to remote
(off-process) memory. For details on these features, if you&rsquo;re
interested, I recommend the books <a href=https://mitpress.mit.edu/books/using-mpi-third-edition>Using
MPI</a> and
<a href=https://mitpress.mit.edu/books/using-advanced-mpi>Using Advanced
MPI</a>. See also
<a href=https://htor.inf.ethz.ch/teaching/mpi_tutorials/>Torsten Hoefler&rsquo;s
tutorials</a>.</p><h2 id=language-bindings-and-libraries>Language bindings and libraries
<a class=anchor href=#language-bindings-and-libraries>#</a></h2><p><a href=https://julialang.org>Julia</a> has MPI bindings in
<a href=https://github.com/JuliaParallel/MPI.jl>MPI.jl</a>, and
distributed arrays in the
<a href=https://github.com/barche/MPIArrays.jl>MPIArrays.jl</a> package.</p><p>As we&rsquo;ve seen in this course, Python has wrappers via
<a href=https://mpi4py.readthedocs.io/en/stable/>mpi4py</a>. For distributed
array computation, look at <a href=https://dask.org>dask</a>.</p><p>For parallel sparse linear algebra, and PDE solvers,
<a href=https://www.mcs.anl.gov/petsc>PETSc</a> and
<a href=https://trilinos.github.io>Trilinos</a> are robust and well-supported
libraries. PETSc has mature Python bindings provided by <a href=https://teaching.wence.uk/comp4187/lectures/mpi/petsc4py/>petsc4py</a>.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/comp4187/commit/9271858aa631161252031b9cd38d4da1c5c802a0 title="Last modified by Lawrence Mitchell | February 14, 2024" target=_blank rel=noopener><img src=/comp4187/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 14, 2024</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/comp4187/edit/main/site/content/lectures/mpi/advanced.md target=_blank rel=noopener><img src=/comp4187/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence@wence.uk>Lawrence Mitchell</a>, <a href=https://annereinarz.github.io>Anne Reinarz</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/comp4187/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#some-pointers-to-more-advanced-features-of-mpi>Some pointers to more advanced features of MPI</a><ul><li><a href=#communicators>Communicator manipulation</a><ul><li><a href=#duplicating-communicators>Duplicating communicators</a></li><li><a href=#splitting-communicators-into-subgroups>Splitting communicators into subgroups</a></li></ul></li><li><a href=#further-features-and-details>Further features and details</a><ul><li><a href=#file-io>File IO</a></li><li><a href=#profiling-interfacehttpswwwmpi-forumorgdocsmpi-31mpi31-reportnode357htmnode357><a href=https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node357.htm#Node357>Profiling interface</a></a></li><li><a href=#one-sided-messaging-and-remote-memory-access>One-sided messaging and Remote Memory Access</a></li></ul></li><li><a href=#language-bindings-and-libraries>Language bindings and libraries</a></li></ul></li></ul></nav></aside></main></body></html>