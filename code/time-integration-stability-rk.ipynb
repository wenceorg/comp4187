{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability of timestepping schemes\n",
    "\n",
    "You already saw in CMIII that for some problems if we make the timestep too large in the forward Euler method, we don't get a valid solution. Let's remind ourselves with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stiff problems\n",
    "\n",
    "Let's consider a seemingly benign linear system of equations (this example is taken from Iserles' book, section 4.1)\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\dot u_0\\\\\n",
    "\\dot u_1\\\\\n",
    "\\end{bmatrix}\n",
    "= \\overbrace{\\begin{bmatrix}\n",
    "-100 & 1\\\\\n",
    "0 & -1/10\\\\\n",
    "\\end{bmatrix}}^{\\Lambda}\n",
    "\\begin{bmatrix}\n",
    "u_0\\\\\n",
    "u_1\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Factorising the matrix $\\Lambda = V D V^{-1}$, it is easy to show that the exact solution at time $t$ obeys\n",
    "\n",
    "$$\n",
    "u(t) = e^{-100 t} x_1 + e^{-t/10} x_2\n",
    "$$\n",
    "\n",
    "where $x_1$ and $x_2$ depend on the initial conditions, but not $t$.\n",
    "\n",
    "For large $t$, the behaviour of the system is controlled by the slowest decaying of the two exponentials. That is, for all practical purposes, we can write:\n",
    "\n",
    "$$\n",
    "u(t) \\approx e^{-t/10} x_2, t > 0.\n",
    "$$\n",
    "\n",
    "Let's try out explicit Euler for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "pyplot.style.use(\"ggplot\")\n",
    "\n",
    "from scipy.linalg import expm\n",
    "\n",
    "class linear(object):\n",
    "    def __init__(self, A):\n",
    "        self.A = numpy.asarray(A)\n",
    "    def f(self, t, u):\n",
    "        return self.A @ u\n",
    "    __call__ = f\n",
    "    def explicit(self, t, u):\n",
    "        return numpy.zeros_like(u)\n",
    "    def J(self, t, u):\n",
    "        return self.A\n",
    "    def exact(self, t, u_0):\n",
    "        t = numpy.array(t, ndmin=1)\n",
    "        return [numpy.real_if_close(expm(self.A*s) @ u_0) for s in t]\n",
    "    \n",
    "test = linear([[-100, 1], [0, -1/10]])\n",
    "\n",
    "u_0 = numpy.asarray([1, 1])\n",
    "\n",
    "def ode_euler(f, u_0, h=0.1, T=1):\n",
    "    u = numpy.array(u_0)\n",
    "    t = 0\n",
    "    thist = [t]\n",
    "    uhist = [u_0]\n",
    "    while t < T:\n",
    "        h = min(h, T - t)\n",
    "        u = u + h * f(t, u)\n",
    "        t += h\n",
    "        thist.append(t)\n",
    "        uhist.append(u.copy())\n",
    "    return numpy.asarray(thist), numpy.asarray(uhist)\n",
    "\n",
    "thist, uhist = ode_euler(test, u_0, h=1/100, T=1)\n",
    "pyplot.figure()\n",
    "\n",
    "pyplot.semilogy(thist, uhist, \"o\", linestyle=\"solid\", label='Forward Euler')\n",
    "pyplot.semilogy(thist, test.exact(thist, u_0), label='exact');\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "When the timestep is very small, we get a reasonably accurate answer. However, when the timestep gets big, suddently the numerical solution blows up.\n",
    "\n",
    "This is due to the small _stability domain_ of the explicit Euler method. As we will see below, the problem is only stable in a small region in the complex plane.\n",
    "\n",
    "This constraint forces us to _artificially_ constrain the step size to avoid instabilities in the numerical solution.\n",
    "\n",
    "We will informally call these problems, where the timestep is small for stability, rather than accuracy, reasons *stiff*\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Show that the explicit Euler solution after $n$ steps of size $h$ can be written as\n",
    "$$\n",
    "u_n = (1 - 100h)^n x_1 + (1 - \\frac{1}{10}h)^n x_2\n",
    "$$\n",
    "2. Use this to determine the appropriate maximum step size for the integration.\n",
    "3. Confirm your analysis with some experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An implicit method\n",
    "\n",
    "Now let's consider the same problem, only this time we'll use backward Euler, an implicit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_beuler(eqn, u_0, h=0.1, T=1):\n",
    "    A = eqn.A\n",
    "    u = numpy.array(u_0)\n",
    "    t = 0\n",
    "    thist = [t]\n",
    "    uhist = [u_0]\n",
    "    while t < T:\n",
    "        h = min(h, T - t)\n",
    "        # u <- (I - h A)^{-1} u\n",
    "        u = numpy.linalg.solve(numpy.eye(len(A)) - h*A, u)\n",
    "        t += h\n",
    "        thist.append(t)\n",
    "        uhist.append(u.copy())\n",
    "    return numpy.asarray(thist), numpy.asarray(uhist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thist, uhist = ode_beuler(test, u_0, h=1, T=10)\n",
    "pyplot.figure()\n",
    "\n",
    "pyplot.semilogy(thist, uhist, \"o\", linestyle=\"solid\", label='Backward Euler')\n",
    "pyplot.semilogy(thist, test.exact(thist, u_0), label='exact');\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take large timesteps, and only need to make $h$ small if we want higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear stability\n",
    "\n",
    "Why is this?\n",
    "\n",
    "To answer this, we consider the behaviour of the integrator on a linear test problem, the *Dahlquist test equation*:\n",
    "\n",
    "$$\n",
    "\\dot u = \\lambda u \\quad \\lambda \\in \\mathbb{C}.\n",
    "$$\n",
    "\n",
    "When taking a step of length $h$, this equation has the exact solution\n",
    "\n",
    "$$\n",
    "u(h) = u_0 e^{\\overbrace{\\lambda h}^z} = u_0 e^{\\mathfrak{R} z}(\\cos \\mathfrak{I} z + i\\sin\\mathfrak{I} z),\n",
    "$$\n",
    "where we wrote $\\mathbb{C} \\ni z = \\lambda h$.\n",
    "\n",
    "This problem is _physically_ stable whenever the real part of $z$ (and hence $\\lambda$) is less than zero: $\\mathfrak{R} z \\le 0$.\n",
    "\n",
    "### Aside\n",
    "\n",
    "There is also theory surrounding _nonlinear_ stability, but we will not cover it in this course.\n",
    "\n",
    "## Stability regions\n",
    "\n",
    "Let us now consider applying the timestepping schemes we've already encountered to the test equation to see how they behave.\n",
    "\n",
    "### Explicit Euler\n",
    "\n",
    "$$\n",
    "u(h) = \\underbrace{(1 + \\lambda h)}_{R(z)}u_0.\n",
    "$$\n",
    "\n",
    "Repeated application of the scheme results in:\n",
    "\n",
    "$$\n",
    "u(mh) = R(z)^m u_0.\n",
    "$$\n",
    "\n",
    "This scheme is convergent (producing a bounded $u(mh)$ in the limit $m \\to \\infty$) only if\n",
    "\n",
    "$$\n",
    "|R(z)| \\le 1.\n",
    "$$\n",
    "\n",
    "### Implicit Euler\n",
    "\n",
    "Performing a similar calculation we obtain\n",
    "\n",
    "$$\n",
    "R(z) = \\frac{1}{1 - z}.\n",
    "$$\n",
    "\n",
    "### Trapezoidal rule/implicit midpoint\n",
    "\n",
    "$$\n",
    "R(z) = \\frac{1 + z/2}{1 - z/2}.\n",
    "$$\n",
    "\n",
    "### Definitions\n",
    "The function $R(z)$ is called the *stability function* of the method, the set\n",
    "\n",
    "$$\n",
    "S = \\{ z \\in \\mathbb{C} \\colon |R(z)| \\le 1 \\}\n",
    "$$\n",
    "\n",
    "is called the *stability domain*.\n",
    "\n",
    "## Stability plots\n",
    "\n",
    "Visualising the stability domains, by plotting $|R(z)|$ is an insightful way of comparing methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stability(x, y, R, label):\n",
    "    pyplot.figure()\n",
    "    C = pyplot.contourf(x, y, numpy.abs(R), numpy.linspace(0, 1, 10), cmap=pyplot.cm.coolwarm)\n",
    "    \n",
    "    pyplot.colorbar(C, ticks=numpy.linspace(0, 1, 10))\n",
    "    pyplot.contour(x, y, numpy.abs(Rz), numpy.linspace(0, 1,4), colors='k')\n",
    "    pyplot.xlabel(\"Re (z)\")\n",
    "    pyplot.ylabel(\"Im (z)\")\n",
    "    pyplot.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.linspace(-3, 3)\n",
    "x, y = numpy.meshgrid(x, x)\n",
    "z = x + 1j*y\n",
    "\n",
    "Rs = [(\"Forward Euler\", 1 + z),\n",
    "      (\"Backward Euler\", 1/(1 - z)),\n",
    "      (\"Implicit Midpoint\", (1 + z/2)/(1 - z/2))]\n",
    "\n",
    "Rs = [(\"Heun\", 1 + z * (1 + z/2))]\n",
    "for label, Rz in Rs:\n",
    "    plot_stability(x, y, Rz, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "z = numpy.linspace(-2, 1)\n",
    "pyplot.plot(z, 1 + z*(1 + z/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "While the physical problem is stable whenever $\\mathfrak{R} \\lambda \\le 0$, the same is not true of all our methods. The explicit Euler method is only stable in a small region of the left half plane, implicit Euler is stable in the entire left half plane, and more, implicit midpoint is stable in exactly the left half plane.\n",
    "\n",
    "A question naturally arises? Why would one ever use an explicit method? Can you think of any reasons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "### A-stability\n",
    "\n",
    "A method is _A-stable_ if the stability domain\n",
    "\n",
    "$$\n",
    "S = \\{z \\in \\mathbb{C} \\colon |R(z)| \\le 1\\}\n",
    "$$\n",
    "\n",
    "contains the _entire_ left half plane\n",
    "\n",
    "$$\n",
    "\\mathfrak{R} z \\le 0.\n",
    "$$\n",
    "\n",
    "This means that we can take arbitrarily large timesteps ($h \\to \\infty$) without the method becoming unstable (diverging) for any problem that is physically stable. Note that this says nothing about the _accuracy_ of the method.\n",
    "\n",
    "No explicit method is A-stable.\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Show that the midpoint method and implicit Euler really do contain the entire left half plane. Hint: multiply through by an appropriate choice of $1$, write $z = a + bi$ and show that $|R(z)| \\le 1$ whenever $a \\le 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher order methods\n",
    "\n",
    "So far, we've seen the implicit and explicit Euler methods which are only first order accurate, and the implicit midpoint method which is second order accurate. We might, especially if we have an accurate spatial discretisation, want more accuracy in our time integrator. We will now look at some methods that offer this. Certainly if the timestep size is no longer limited by stability requirements (for example when using an implicit integrator), it makes sense to take big steps, but then we need a high order method for accuracy.\n",
    "\n",
    "In this section we will look at single-step Runge-Kutta methods (covered briefly at the very end of CMIII).\n",
    "\n",
    "Many of these methods offer a clever way of estimating the local error accumulation at each step, and we will look at how this can be used to implement an adaptive timestepping scheme.\n",
    "\n",
    "High-order timestepping schemes also have a higher cost of evaluation (as we shall see), and we'll discuss how to compare different schemes in a fair way using _work-precision_ diagrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Runge-Kutta methods](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods)\n",
    "\n",
    "These are a class of _single-step_ methods. That is, we advance a solution $u_0$ from $t_0$ to $t_1$ using $u_0$ as an initial value, producing $u_1$, then when advancing from $t_1$ to $t_2$ we forget about $u_0$ and use $u_1$ as an initial value.\n",
    "\n",
    "Recall that the single-step methods we have looked at so far (Euler and midpoint schemes) are all obtained by formally solving the ODE with integration and then approximating the integral.\n",
    "\n",
    "$$\n",
    "\\dot u = f(t, u), u(t_0) = u_0\n",
    "$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$\n",
    "u(t) = u(t_0) + \\int_{t_0}^t f(\\tau, u) \\text{d}\\tau.\n",
    "$$\n",
    "\n",
    "A single step is then obtained by setting $t = t_0 + h$ and writing\n",
    "\n",
    "$$\n",
    "\\int_{t_0}^{t_0 + h} f(\\tau, u) \\text{d}\\tau = h\\int_0^1 f(t_0 + h\\tau, y(t_0 + h\\tau))\\text{d}\\tau.\n",
    "$$\n",
    "\n",
    "We now _approximate_ the integral with a sum (using a quadrature scheme, but let's sweep that under the carpet for now), by picking a set of points $\\{\\xi_i\\}_{i=1}^{N} \\in [0, 1]$ and weights $\\{w_i\\}_{i=1}^{N} \\in \\mathbb{R}$\n",
    "\n",
    "$$\n",
    "\\int_0^1 f(t_0 + h\\tau, y(t_0 + h\\tau))\\text{d}\\tau \\approx \\sum_{i=1}^{N} w_i f(t_0 + \\xi_i h, y(t_0 + \\xi_i h)).\n",
    "$$\n",
    "\n",
    "Giving us our update formula:\n",
    "$$\n",
    "u_{n+1} = u_n + h \\sum_{i=1}^{N} w_i f(t_n + \\xi_i h, u(t_n + \\xi_i h)).\n",
    "$$\n",
    "\n",
    "For example, explicit Euler was obtained with $N=1$, $\\{\\xi_i\\} = \\{0\\}$, and $\\{w_i\\} = \\{1\\}$.\n",
    "\n",
    "Our challenge, for more general schemes, is to figure out how to approximate the (unknown) $u$s, we only know the initial value $u(t_0)$.\n",
    "\n",
    "### Explicit Runge-Kutta methods\n",
    "\n",
    "Write $Y_i$ for our numerical approximation to $u(t_0 + \\xi_i h)$. _Explicit_ Runga-Kutta methods compute $Y_i, i = 2,\\dots,N$ as a linear combination of $Y_j, j < i$.\n",
    "\n",
    "That is, we write\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y_1 &= u_n\\\\\n",
    "Y_2 &= u_n + h a_{2, 1} f(t_n, Y_1)\\\\\n",
    "Y_3 &= u_n + h(a_{3,1} f(t_n, Y_1) + a_{3,2} f(t_n + c_2 h, Y_2)\\\\\n",
    "&\\vdots\\\\\n",
    "Y_N &= u_n + h \\sum_{i=1}^{N-1} a_{N, i} f(t_n + c_i h, Y_i)\\\\\n",
    "u_{n+1} &= u_n + h \\sum_{i=1}^{N} b_{i} f(t_n + c_i h, Y_i).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Each of these intermediate steps is termed a *stage*, and so we just wrote down an $N$-stage Runge-Kutta method.\n",
    "\n",
    "For an explicit method, the _RK matrix_ (or *coefficients*)\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a_{1, 1} & a_{1, 2} & \\dots & a_{1, N}\\\\\n",
    "a_{2, 1} & a_{2, 2} & \\dots & a_{2, N}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{N, 1} & a_{N, 2} & \\dots & a_{N, N}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "is strictly lower-triangular ($a_{i, j} = 0, j \\ge i$).\n",
    "\n",
    "If we further write the _RK weights_ (or *completion weights*)\n",
    "\n",
    "$$\n",
    "b = \\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and the _RK nodes_ (or *abscissa*)\n",
    "\n",
    "$$\n",
    "c = \\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "\\vdots \\\\\n",
    "c_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "the entire scheme can be compactly represented in a _Butcher tableau_ \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c}\n",
    "c & A\\\\\n",
    "\\hline\n",
    " & b^T\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "(named after [John Butcher](https://en.wikipedia.org/wiki/John_C._Butcher) who developed much of the theory underpinning the analysis and design of Runge-Kutta methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining $A$, $b$, and $c$\n",
    "\n",
    "For low order methods, our first thought is to Taylor expand everything around $(t_n, u_n)$ and then equate terms with the Taylor expansion of the exact solution.\n",
    "\n",
    "This is doable for $N=2$, but already becomes tremendously labourious at $N=3$. Fortunately, other people (notably Butcher) have developed graph theoretical tools to construct, and analyse the properties of, Runge-Kutta methods. Here, we will only the resulting methods.\n",
    "\n",
    "One particular point to note is that the conditions on the entries in $A$, $b$, and $c$ do not uniquely specify the coefficients, so it is possible to come up with multiple different methods with the same number of stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all that, let's write some code to do explict RK integration. The Butcher tableau is not just a useful mathematical device, it allows us to write generic code for RK integration (rather than one method for explicit Euler, one for implicit Euler, one for midpoint, etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButcherTable(object):\n",
    "    def __init__(self, A, b, order=None, btilde=None):\n",
    "        self.A = numpy.asarray(A)\n",
    "        self.b = numpy.asarray(b)\n",
    "        self.order = order\n",
    "        self.btilde = numpy.asarray(btilde) if btilde else None\n",
    "        self.c = self.A.sum(axis=1)\n",
    "\n",
    "    def _repr_latex_(self):\n",
    "        rows, cols = self.A.shape\n",
    "        strs = [\"$$\", r\"\\left[\", r\"\\begin{array}{c|%s}\" % (\"c\"*cols)]\n",
    "        for r in range(rows):\n",
    "            row = \" & \".join(map(str, [self.c[r]] + list(self.A[r, :])))\n",
    "            strs.append(row)\n",
    "            strs.append(r\"\\\\\")\n",
    "        strs.append(r\"\\hline\")\n",
    "        \n",
    "        strs.append(\"&\")\n",
    "        strs.append(\" & \".join(map(str, self.b)))\n",
    "        strs.append(r\"\\\\\")\n",
    "        if self.btilde is not None:\n",
    "            strs.append(\"&\" + \" & \".join(map(str, self.btilde)))\n",
    "        strs.extend([r\"\\end{array}\", r\"\\right]\", \"$$\"])\n",
    "        return \"\\n\".join(strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler = ButcherTable([[0]], [1], order=1); euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beuler = ButcherTable([[1]], [1], order=1); beuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midpoint = ButcherTable([[1/2]], [1], order=2); midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_rk_explicit(f, u_0, table, h=0.1, T=1):\n",
    "    A = table.A\n",
    "    b = table.b\n",
    "    c = A.sum(axis=1)\n",
    "    N = len(c)\n",
    "    for i in range(N):\n",
    "        if any(A[i, i:] > 0):\n",
    "            raise ValueError(\"Only for explicit methods\")\n",
    "    t = 0\n",
    "    u_0 = numpy.asarray(u_0)\n",
    "    u = u_0.copy()\n",
    "    hist = [(t, u.copy())]\n",
    "    fY = numpy.zeros(u_0.shape + (N, ))\n",
    "    while t < T:\n",
    "        h = min(h, T - t)\n",
    "        for i in range(N):\n",
    "            Yi = u.copy()\n",
    "            for j in range(i):\n",
    "                Yi += h * A[i, j] * fY[:, j]\n",
    "            fY[:, i] = f(t + h*c[i], Yi)\n",
    "        u += h * fY @ b\n",
    "        t += h\n",
    "        hist.append((t, u.copy()))\n",
    "    ts, us = zip(*hist)\n",
    "    return numpy.asarray(ts), numpy.asarray(us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = linear([[0, 1.0], [-1.0, 0.0]])\n",
    "\n",
    "u_0 = numpy.asarray([1.0, 0])\n",
    "\n",
    "ts, us = ode_rk_explicit(test, u_0, euler, h=0.1, T=15)\n",
    "pyplot.figure()\n",
    "pyplot.plot(ts, us, label=\"Forward Euler\");\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine multiple explicit Euler steps for a higher order method, [Heun's method](https://en.wikipedia.org/wiki/Heun%27s_method) (also called \"modified Euler\" or \"explicit trapezoid\"). This has Butcher tableau\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{c|cc}\n",
    "0 & 0 & 0\\\\\n",
    "1 & 1 & 0\\\\\n",
    "\\hline\n",
    "  & 1/2 & 1/2\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heun = ButcherTable(numpy.asarray([[0, 0], [1, 0]]), numpy.asarray([1/2, 1/2]), order=2); heun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts, us = ode_rk_explicit(test, u_0, heun, h=0.1, T=15)\n",
    "pyplot.figure()\n",
    "pyplot.plot(ts, us, label=\"Heun\");\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps _the_ most famous RK method is the explicit 4-th order RK4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rk4 = ButcherTable(numpy.asarray([[0, 0, 0, 0],\n",
    "                                  [1/2, 0, 0, 0],\n",
    "                                  [0, 1/2, 0, 0],\n",
    "                                  [0, 0, 1, 0]]),\n",
    "                   numpy.asarray([1/6, 1/3, 1/3, 1/6]), order=4); rk4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts, us = ode_rk_explicit(test, u_0, rk4, h=0.1, T=15)\n",
    "pyplot.figure()\n",
    "pyplot.plot(ts, us, label=\"RK4\");\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Derive an expression for the stability function $R(z)$ for RK methods by applying one step to the Dahlquist test equation\n",
    "2. Extend the `ButcherTable` class with a method to plot the method's stability region.\n",
    "3. What do you see when comparing the stability of Heun and RK4 to that of explicit Euler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing methods\n",
    "\n",
    "All other things being equal, a higher-order method is likely a superior choice, since it will converge faster to the exact solution with bigger timesteps. For example, let's compute the convergence rate for the explicit Euler, BS3, RK4, and DP5 methods on our linear osciliatory test problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def mms(eqn, integrator, u_0, h=0.1, T=15):\n",
    "    thist, uhist = integrator(eqn, u_0, h=h, T=T)\n",
    "    return numpy.linalg.norm(uhist[-1] - eqn.exact(thist[-1], u_0), numpy.inf)\n",
    "\n",
    "eqn = linear(numpy.array([[0, 1],\n",
    "                          [-1, 0]]))\n",
    "u_0 = numpy.array([.75, 0])\n",
    "\n",
    "hs = numpy.array([1/2**i for i in range(2, 10)])\n",
    "pyplot.figure()\n",
    "method_errors = {}\n",
    "for name, method in [(\"Forward Euler\", partial(ode_rk_explicit, table=euler)),\n",
    "                     (\"Heun\", partial(ode_rk_explicit, table=heun)),\n",
    "                     (\"RK4\", partial(ode_rk_explicit, table=rk4))]:\n",
    "    errors = []\n",
    "    for h in hs:\n",
    "        error = mms(eqn, method, u_0, h=h, T=8)\n",
    "        errors.append(error)\n",
    "    method_errors[name] = numpy.asarray(errors)\n",
    "    pyplot.loglog(hs, errors, \".\", label=name)\n",
    "    \n",
    "pyplot.loglog(hs, 1.5*hs, label=\"$\\mathcal{O}(h)$\")\n",
    "pyplot.loglog(hs, 2*hs**2, label=\"$\\mathcal{O}(h^2)$\")\n",
    "pyplot.loglog(hs, 0.1*hs**4, label=\"$\\mathcal{O}(h^4)$\")\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"h\")\n",
    "pyplot.ylabel(\"$e_{n,h}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What timestep would you need for Heun's method and forward Euler respectively to obtain an error of $10^{-10}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work precision diagrams\n",
    "\n",
    "This comparison is slightly unfair, since when we are only looking at the error, higher-order methods will clearly perform better for the same timestep. Especially on this oscillatory test problem that has a very smooth solution.\n",
    "\n",
    "A fairer method of comparison is to use _work precision_ diagrams. In these, we plot some proxy for the total _cost_ of a method against the obtained error. For explicit methods, the proxy is usually the _number of function evaluations_ (since it avoids arguments about inefficiencies in implementation) though _time to solution_ is also valid.\n",
    "\n",
    "With this in mind, we can replot our convergence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work(hs, table, T=15):\n",
    "    A = table.A\n",
    "    b = table.b\n",
    "    stages, _ = A.shape\n",
    "    if numpy.allclose(A[-1, :], b):\n",
    "        stages -= 1\n",
    "    steps = T/hs\n",
    "    return steps * stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = partial(work, hs, T=15)\n",
    "pyplot.figure()\n",
    "for name, table in [(\"Forward Euler\", euler),\n",
    "                    (\"Heun\", heun),\n",
    "                    (\"RK4\", rk4)]:\n",
    "    pyplot.loglog(cost(table), method_errors[name], marker=\"o\", label=name)\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"Number of function evaluations\")\n",
    "pyplot.ylabel(\"Error\")\n",
    "pyplot.title(\"Error vs Cost\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. This looks like the high-order method still always wins. But can you actually increase the step size sufficiently that you can extrapolate to the low-error case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error control\n",
    "\n",
    "In CMIII you saw a relatively simple method for adaptive timestepping. Given a single timestepping scheme with step length $h$, the error can be estimated by performing two steps with step length $h/2$ and computing the difference in the resulting guess.\n",
    "\n",
    "This is rather wasteful, in that we need to perform three time integrations to take one step.\n",
    "\n",
    "For higher-order RK schemes, a more sophisticated approach is possible. Since the constraints on the coefficients in the Butcher table can admit more than one possible solution it is often possible to use the same set of stages with two different sets of *completion weights* (the $b$ vectors) to produce two updates of different order effectively \"for free\". The error can then be estimated by computing the difference between the two steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error estimation with embedded methods\n",
    "\n",
    "Let us suppose we have an RK method\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{c|c}\n",
    "c & A\\\\\n",
    "\\hline\n",
    "& b^T\\\\\n",
    "& \\tilde{b}^T\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Where the completion weights $b^T$ produce a method of order $p$, and the weights $\\tilde{b}^T$ produce a mthod of order $\\tilde{p} = p - 1$. We then have, from a starting point $u_n$, two candidate solutions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u_{n+1} &= u_n + h b^T f(Y)\\\\\n",
    "\\tilde{u}_{n+1} &= u_n + h \\tilde{b}^T f(Y)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $f(Y)$ is the vector of intermediate function evaluations.\n",
    "\n",
    "We can now estimate the error in a single step by\n",
    "\n",
    "$$\n",
    "e(h) = \\| u_{n+1} - \\tilde{u}_{n+1} \\| = \\| h (b - \\tilde{b})^T f(Y) \\| \\in \\mathcal{O}(h^p)\n",
    "$$\n",
    "\n",
    "Given some desired tolerance $\\epsilon$, we want to find $h_*$ such that\n",
    "\n",
    "$$\n",
    "e(h_*) < \\epsilon\n",
    "$$\n",
    "\n",
    "Since $e(h) \\in \\mathcal{O}(h^p)$ we can write $e(h) = c h^p$, then our requirement on $h_*$ means we want\n",
    "\n",
    "$$\n",
    "c h_*^p < \\epsilon\n",
    "$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\n",
    "h_* < \\left(\\frac{\\epsilon}{c}\\right)^{1/p}.\n",
    "$$\n",
    "\n",
    "We estimate the unknown $c$ by using our error estimate:\n",
    "\n",
    "$$\n",
    "e(h) = c h^p \\Leftrightarrow c = \\frac{e(h)}{h^p}\n",
    "$$\n",
    "\n",
    "and substituting in\n",
    "\n",
    "$$\n",
    "h^* < h \\left(\\frac{\\epsilon}{e(h)}\\right)^{1/p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BS3\n",
    "\n",
    "A low-order method which implements this embedded idea is the [Bogacki-Shampine](https://en.wikipedia.org/wiki/Bogacki–Shampine_method) method of order 3, which comes with an embedded 2nd order method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs3 = ButcherTable([[0, 0, 0, 0],\n",
    "                    [1/2, 0, 0, 0],\n",
    "                    [0, 3/4, 0, 0],\n",
    "                    [2/9, 1/3, 4/9, 0]],\n",
    "                   [2/9, 1/3, 4/9, 0], # order 3\n",
    "                   order=3,\n",
    "                   btilde=[7/24, 1/4, 1/3, 1/8]) # order 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this method has 4 stages, it only requires 3 function evaluations, since if we stare closely we can see that the last stage has the same coefficients as the order 3 completion formula (so we can reuse the last stage for the next timestep). This is termed the _First Same as Last_ or FSAL property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODE45\n",
    "\n",
    "The default ODE solver in MATLAB (amongst others) is the 5th order [Dormand-Prince](https://en.wikipedia.org/wiki/Dormand–Prince_method) method (with a 4th order embedded method). One can vaguely imagine that this table was not computed by frantically Taylor expanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp45 = ButcherTable([[0, 0, 0, 0, 0, 0, 0],\n",
    "                     [1/5, 0, 0, 0, 0, 0, 0],\n",
    "                     [3/40, 9/40, 0, 0, 0, 0, 0],\n",
    "                     [44/45, -56/15, 32/9, 0, 0, 0, 0],\n",
    "                     [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0, 0],\n",
    "                     [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0, 0],\n",
    "                     [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]],\n",
    "                    [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0], \n",
    "                    order=5,# 5th order\n",
    "                    btilde=[5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40]) # 4th order\n",
    "dp45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this method also has the FSAL property for the 5th order completion formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive error control\n",
    "\n",
    "Let us look at an example use of adaptive error control. The step-size acceptance here is rather simple, for a more thorough treatment, see [Hairer & Wanner (1993), pages 167-168](https://www.springer.com/gp/book/9783540566700).\n",
    "\n",
    "We solve the coupled chemical reaction system known as the \"Brusselator\" [Lefever & Nicolis (1971)](https://doi.org/10.1016/0022-5193(71)90054-3).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dot{u}_1 &= A + u_1^2 u_2 - (B+1)u_1\\\\\n",
    "\\dot{u}_2 &= Bu_1 - u_1^2 u_2\\\\\n",
    "A &= 1\\\\\n",
    "B &= 3\\\\\n",
    "u_1(0) &= 1.5\\\\\n",
    "u_2(0) &= 3.0\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class brusselator(object):\n",
    "    def __init__(self,  A, B):\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "    \n",
    "    def __call__(self, t, u):\n",
    "        u1, u2 = u\n",
    "        A = self.A\n",
    "        B = self.B\n",
    "        return numpy.asarray([A + u1**2*u2 - (B + 1)*u1, B*u1 - u1**2*u2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the error control we will use the embedded method in our RK scheme to calculate an error $\\|u - \\tilde{u}\\|$. If this is larger than some specified $\\epsilon$ we should reject the step. Additionally, an optimal $h$ for the next step (irrespective of whether we accepted the step or not) is chosen by using the inequality above.\n",
    "\n",
    "We set:\n",
    "\n",
    "$$\n",
    "h_\\text{next} = 0.9 * h_\\text{current} \\left( \\frac{\\| u - \\tilde{u}\\|}{\\epsilon}\\right)^{-1/p}\n",
    "$$\n",
    "\n",
    "where $p$ is the order of the method. The factor of 0.9 is a safety factor to ensure we have some wiggle room in our optimal step size estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_rk_explicit_adaptive(f, u_0, table, eps=0.1, h_0=0.1, T=1):\n",
    "    \"\"\"Integrate f using an explicit RK scheme and adaptive timestepping.\n",
    "    \n",
    "    :arg f: The RHS a callable f(t, u) -> udot\n",
    "    :arg u_0: initial value for u\n",
    "    :arg table: a :class:`ButcherTable`.\n",
    "    :arg eps: target local error.\n",
    "    :arg h_0: Initial step size.\n",
    "    :arg T: Final time.\"\"\"\n",
    "    A = table.A\n",
    "    b = table.b\n",
    "    btilde = table.btilde\n",
    "    if btilde is None:\n",
    "        raise ValueError(\"Provided scheme %r does not have embedded scheme\" % table)\n",
    "    order = table.order\n",
    "    c = A.sum(axis=1)\n",
    "    N = len(c)\n",
    "    t = 0\n",
    "    u_0 = numpy.asarray(u_0)\n",
    "    u = u_0.copy()\n",
    "    hist = [(t, u.copy(), h_0)]\n",
    "    fY = numpy.zeros(u_0.shape + (N, ))\n",
    "    h = h_0\n",
    "    while t < T:\n",
    "        accept = True\n",
    "        h = min(h, T - t)\n",
    "        fY[:] = 0\n",
    "        # Standard RK stages\n",
    "        for i in range(N):\n",
    "            Yi = u.copy()\n",
    "            for j in range(i):\n",
    "                Yi += h * A[i, j] * fY[:, j]\n",
    "            fY[:, i] = f(t + h*c[i], Yi)  \n",
    "        # Error estimation\n",
    "        ulo_up = h * fY @ btilde\n",
    "        u_up = h * fY @ b\n",
    "        err = numpy.linalg.norm(u_up - ulo_up)\n",
    "        relerr = err / eps\n",
    "        if relerr > 1: # error bigger than eps\n",
    "            # The step was too big\n",
    "            accept = False\n",
    "        if relerr > 0:\n",
    "            next_h = h * 0.9*relerr**(-1/order)\n",
    "        else:\n",
    "            # Some large number\n",
    "            next_h = h*2\n",
    "        if accept:\n",
    "            # Step was fine (error was controlled), so use it.\n",
    "            u += u_up\n",
    "            t += h\n",
    "            hist.append((t, u.copy(), h))\n",
    "        # New step\n",
    "        h = next_h\n",
    "    ts, us, hs = zip(*hist)\n",
    "    return numpy.asarray(ts), numpy.asarray(us), numpy.asarray(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = numpy.asarray([1.5, 3.0])\n",
    "eqn = brusselator(A=1, B=4)\n",
    "\n",
    "thist, uhist, hhist = ode_rk_explicit_adaptive(eqn, u, bs3, eps=1e-3, h_0=0.1, T=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(thist, uhist);\n",
    "pyplot.figure()\n",
    "pyplot.plot(thist, hhist);\n",
    "pyplot.xlabel(\"t\")\n",
    "pyplot.ylabel(\"h\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Use a small timestep, high-order integrator to compute an \"exact\" solution. Use this to produce MMS convergence plots for the BS3 and DP45 methods using this adaptive error control mechanism. How do you need to vary $\\epsilon$ to get the convergence order you expect?\n",
    "\n",
    "2. Using the same high-order \"exact\" solution, produce a work-precision diagram comparing fixed step-size methods against adaptive methods. Are the adaptive methods always superior in terms of work for a given accuracy? You may find it useful to augment the `brusselator` class with a count object that keeps track of the number of function evaluations (don't forget to create a fresh object for each integration though!).\n",
    "\n",
    "3. Compare, using a work-precision diagram, the cost of this \"embedded\" adaptivity scheme with the \"try with h/2 and compare\" approach of CMIII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to stiff problems\n",
    "\n",
    "## Implicit Runge-Kutta methods\n",
    "\n",
    "We have already seen that we will want an implicit, rather than explicit, method when taking large timesteps. The same formalism for expressing Runge-Kutta methods is also applicable to implicit ones. In this case, $A$ is no longer strictly lower-triangular. Popular implicit RK schemes are typically at least lower triangular ($a_{i,j} = 0, j > i$). This way, although we need to solve a system for each stage, we can still do forward-substitutions, and the stage equations become:\n",
    "\n",
    "$$\n",
    "Y_i - h a_{i,i} f(t_n + c_i h, Y_i) = u_n + h \\sum_{j < i} a_{i, j} f(t_n + c_j h, Y_j).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are termed DIRK (_diagonally_ implicit Runge-Kutta) methods. \n",
    "\n",
    "A subclass that is even better (if achievable) is SDIRK (_singly_ diagonally implicit Runge-Kutta) methods. This last implicit class has $a_{i,i} = \\alpha$ for all $i$.\n",
    "\n",
    "These methods are attractive because we can often reuse (part of) the setup cost for solving the systems at each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the _stiff_ equation we first saw\n",
    "\n",
    "$$\n",
    "\\dot{u} = -k(u - \\cos t)\n",
    "$$\n",
    "\n",
    "and try solving it when $k$ is very large (this increases the stiffness).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stiff(object):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "    def f(self, t, u):\n",
    "        return -self.k * (u - numpy.cos(t))\n",
    "    \n",
    "    def explicit(self, t, u):\n",
    "        # This part of the forcing function will be moved to the rhs\n",
    "        return self.k * numpy.cos(t)\n",
    "    \n",
    "    __call__ = f\n",
    "    \n",
    "    def J(self, t, u):\n",
    "        # This is the linearly implicit part of the forcing function (solved for)\n",
    "        # df/du\n",
    "        return numpy.asarray(-self.k).reshape(1, 1)\n",
    "    \n",
    "    def exact(self, t, u_0):\n",
    "        k = self.k\n",
    "        k2p1 = k/(1 + k**2)\n",
    "        return (u_0 - k*k2p1)*numpy.exp(-k*t) + k2p1 * (numpy.sin(t) + k*numpy.cos(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_dirk_linear(eqn, u_0, table, h=0.1, T=1):\n",
    "    A = table.A\n",
    "    b = table.b\n",
    "    order = table.order\n",
    "    c = A.sum(axis=1)\n",
    "    N = len(c)\n",
    "    for i in range(N):\n",
    "        if any(A[i, i+1:] > 0):\n",
    "            raise ValueError(\"Only for diagonally implicit RK schemes\")\n",
    "    t = 0\n",
    "    u_0 = numpy.asarray(u_0).reshape(-1)\n",
    "    u = u_0.copy()\n",
    "    hist = [(t, u.copy())]\n",
    "    fY = numpy.zeros(u_0.shape + (N, ))\n",
    "    while t < T:\n",
    "        h = min(h, T - t)\n",
    "        fY[:] = 0\n",
    "        # Standard RK stages\n",
    "        for i in range(N):\n",
    "            Yi = u.copy()\n",
    "            tstage = t + h*c[i]\n",
    "            for j in range(i):\n",
    "                Yi += h * A[i, j] * fY[:, j]\n",
    "            if A[i, i] != 0:\n",
    "                # Solving\n",
    "                # Yi - h a_ii f(tstage, Yi) = u_n + \\sum_{i<j} h A[i, j] f(t + c[j] h, Y_j)\n",
    "                # f(stage, Yi) splits into linearly implicit part and (non)linear explicit part (moved to rhs)\n",
    "                J = eqn.J(tstage, Yi)\n",
    "                rhs = Yi + h * A[i, i] * eqn.explicit(tstage, Yi)\n",
    "                Yi = numpy.linalg.solve(numpy.eye(J.shape[0]) - h * A[i, i] * J, rhs)\n",
    "            fY[:, i] = eqn(tstage, Yi)\n",
    "        u += h * fY @ b\n",
    "        t += h\n",
    "        hist.append((t, u.copy()))\n",
    "    ts, us = zip(*hist)\n",
    "    return numpy.asarray(ts), numpy.asarray(us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beuler = ButcherTable([[1]], [1], order=1)\n",
    "\n",
    "test = stiff(200)\n",
    "\n",
    "u_0 = numpy.array(0.2)\n",
    "thist, uhist = ode_dirk_linear(test, u_0, beuler, h=0.1, T=12)\n",
    "pyplot.figure()\n",
    "pyplot.plot(thist, uhist, \"o\", linestyle=\"solid\", label='Backward Euler')\n",
    "pyplot.plot(thist, test.exact(thist, u_0), label='exact');\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 stage, 3rd order scheme from Ascher et al. (1997) https://doi.org/10.1137/0732037\n",
    "sdirk43 = ButcherTable([[1/2, 0, 0, 0],\n",
    "                       [1/6, 1/2, 0, 0],\n",
    "                       [-1/2, 1/2, 1/2, 0],\n",
    "                       [3/2, -3/2, 1/2, 1/2]],\n",
    "                      [3/2, -3/2, 1/2, 1/2], order=3)\n",
    "sdirk43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_0 = numpy.array(0.2)\n",
    "\n",
    "test = stiff(k=1000)\n",
    "thist, uhist = ode_dirk_linear(test, u_0, sdirk43, h=0.1, T=20)\n",
    "pyplot.figure()\n",
    "pyplot.plot(thist, uhist, \"o\", linestyle=\"solid\", label='DIRK43')\n",
    "pyplot.plot(thist, test.exact(thist, u_0), label='exact');\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Produce work-precision diagrams for the implicit methods on the stiff equation above. Compare them to using an explicit integrator instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
