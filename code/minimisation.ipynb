{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimisation problems\n",
    "\n",
    "Many problems in scientific computing (and other fields) boil down to solving a minimisation problems\n",
    "\n",
    "$$\n",
    "\\min_{x} F(x; p)\n",
    "$$\n",
    "\n",
    "where $x \\in \\mathbb{R}^n$ are inputs we can control and $p \\in \\mathbb{R}^p$ are some fixed parameters.\n",
    "We might also need to take into account constraints on the inputs $x$, but we'll brush that under the carpet here.\n",
    "\n",
    "For differentiable $F$, minimisation problems are intimately linked to *rootfinding*. That is, we can rephrase the minimisation problem as a problem of finding $x$ such that\n",
    "\n",
    "$$\n",
    "G(x; p) := \\nabla F(x; p) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import pyplot\n",
    "pyplot.style.use('ggplot')\n",
    "import numpy\n",
    "\n",
    "def g0(x):\n",
    "    return x**2 - 2\n",
    "\n",
    "## Used later\n",
    "def dg0(x):\n",
    "    return 2*x\n",
    "\n",
    "x = numpy.linspace(-2,2,100)\n",
    "pyplot.plot(x, g0(x), label=\"$G(x)$\")\n",
    "pyplot.plot(x, dg0(x), label=\"$G_x(x)$\")\n",
    "pyplot.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Neural networks\n",
    "\n",
    "$F$ is the *loss function* which we are trying to minimise, $x$ are the weights in the network, and $p$ are the training data.\n",
    "\n",
    "### Soap films\n",
    "\n",
    "A soap film attached to a wire shape [minimises the surface area](https://en.wikipedia.org/wiki/Minimal_surface) of the film. Picking a coordinate system, and defining $u(x, y, z)$ as the height of the surface at each point in space we have\n",
    "\n",
    "$$\n",
    "G(u) = \\nabla \\cdot \\left(\\frac{\\nabla u}{(1 + |\\nabla u|^2)^{1/2}}\\right) = 0\n",
    "$$\n",
    "\n",
    "### Stationary solutions of time-dependent equations\n",
    "\n",
    "We have a time-dependent PDE\n",
    "\n",
    "$$\n",
    "\\partial_t u - f(t, u) = 0,\n",
    "$$\n",
    "\n",
    "to solve for the steady state, we need to solve\n",
    "\n",
    "$$\n",
    "f(t, u) = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "Some questions immediately come to mind:\n",
    "\n",
    "- **Existence**: does the equation have *at least one* solution?\n",
    "- **Uniqueness**: if a solution exists, is the *only* solution?\n",
    "\n",
    "\n",
    "### Bisection\n",
    "\n",
    "With some mild restrictions on $G := \\nabla F$ (that it is [continuous](https://en.wikipedia.org/wiki/Continuous_function)) we can exploit the intermediate value theorem to find a root.\n",
    "\n",
    "Assume that $G$ changes sign on the interval $[a, b]$, then there must exist a $c \\in [a, b]$ such that $G(c) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def bisect(f, a, b, tol=1e-5, history=None):\n",
    "    midpoint = (a + b) / 2\n",
    "    if b - a < tol:\n",
    "        return midpoint, history\n",
    "    elif abs(f(midpoint)) < tol:\n",
    "        return midpoint, history\n",
    "    if history is not None:\n",
    "        history.append(midpoint)\n",
    "    if f(a)*f(midpoint) < 0:\n",
    "        return bisect(f, a, midpoint, tol=tol, history=history)\n",
    "    else:\n",
    "        return bisect(f, midpoint, b, tol=tol, history=history)\n",
    "    \n",
    "root, history = bisect(g0, -1, 2, history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root, numpy.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.xlabel(\"iteration\")\n",
    "pyplot.ylabel(\"error\")\n",
    "pyplot.semilogy(numpy.abs(history - numpy.sqrt(2)), \"o\", label=\"Convergence\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about some other problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g1(x):\n",
    "    return numpy.exp(-numpy.abs(x)) + numpy.sin(x)\n",
    "\n",
    "def dg1(x):\n",
    "    return numpy.exp(-numpy.abs(x))*(-numpy.sign(x)) + numpy.cos(x)\n",
    "\n",
    "x = numpy.linspace(-2,2,100)\n",
    "pyplot.figure()\n",
    "pyplot.plot(x, g1(x), label=\"$G(x)$\")\n",
    "pyplot.plot(x, dg1(x), label=\"$G_x(x)$\")\n",
    "pyplot.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root, history = bisect(g1, -2.1, 2, history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root, g1(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "x = numpy.linspace(-10,10,100)\n",
    "pyplot.plot(x, g1(x))\n",
    "pyplot.plot(history, g1(history), \"o\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "This is the workhorse of optimisation in machine learning algorithms. Suppose we want to minimise $F$ and have a way of evaluating $G$. Then given an *initial guess* $x_0$ we can produce a new guess for the minimum by walking \"downhill\" in the direction of the gradient:\n",
    "\n",
    "$$\n",
    "x_{i+1} \\gets x_{i} - \\alpha_i G(x_{i}).\n",
    "$$\n",
    "\n",
    "Where $\\alpha_i$ is called the *learning rate* in the ML community, and a \"linesearch damping\" by everyone else. Informally, one chooses $\\alpha_i$ so that we don't \"overshoot\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(g, x, alpha, tol=1e-5, history=None, maxit=100):\n",
    "    x = numpy.asarray(x)\n",
    "    for i in range(maxit):\n",
    "        gx = g(x)\n",
    "        if numpy.linalg.norm(gx) < tol:\n",
    "            return x, gx, i, history\n",
    "        if history is not None:\n",
    "            history.append(x.copy())\n",
    "        x -= alpha*gx\n",
    "    else:\n",
    "        return x, gx, i, history\n",
    "        raise RuntimeError(f\"Didn't converge in {maxit} ({x, gx}) iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1.\n",
    "xstar, gx, i, history = gradient_descent(g1, x0, 0.1, tol=1e-10, maxit=1000, history=[])\n",
    "xstar, g0(xstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.semilogy(numpy.abs(numpy.asarray(history) - history[-1]), \".-\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a (local) minimum, with a sufficiently small damping parameter, gradient descent will find it eventually (but it might take a long time).\n",
    "\n",
    "Gradient descent uses only first order information to compute roots and therefore converges only linearly to solutions. If it is feasible to compute second derivatives, then Newton's method can be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton iteration\n",
    "\n",
    "Suppose that $G$ is sufficiently smooth that we can form its first derivative, then we can write, given a guess $x_0$\n",
    "\n",
    "$$\n",
    "\\tilde{G}(x) = G(x_0) + G'(x_0)(x - x_0) + \\mathcal{O}((x - x_0)^2)\n",
    "$$\n",
    "\n",
    "Since $\\tilde{G}$ is linear, we can explicitly solve for $\\tilde{G}(x) = 0$\n",
    "\n",
    "$$\n",
    "x = x_0 - [G'(x_0)]^{-1}G(x_0)\n",
    "$$\n",
    "\n",
    "This update\n",
    "\n",
    "$$\n",
    "x_{i+1} \\gets x_i - [G'(x_0)]^{-1} G(x_0)\n",
    "$$\n",
    "\n",
    "is a step in a [Newton iteration](https://en.wikipedia.org/wiki/Newton%27s_method).\n",
    "\n",
    "Near a root (if the initial guess is good), this converges *quadratically* to the root. Similarly to gradient descent, it is often augmented with a line search to produce\n",
    "\n",
    "$$\n",
    "x_{i+1} \\gets x_i - \\alpha_i [G'(x_0)]^{-1} G(x_0).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(g, dg, x, tol=1e-5, maxit=100, alpha=1, history=None):\n",
    "    x = numpy.asarray(x)\n",
    "    for i in range(maxit):\n",
    "        gx = g(x)\n",
    "        dgx = dg(x)\n",
    "        if numpy.linalg.norm(gx) < tol:\n",
    "            return x, gx, i, history\n",
    "        if history is not None:\n",
    "            history.append(x.copy())\n",
    "        try:\n",
    "            x -= alpha * numpy.linalg.inv(dgx) @ gx\n",
    "        except numpy.linalg.LinAlgError:\n",
    "            try:\n",
    "                x -= alpha * gx/dgx\n",
    "            except ZeroDivisionError:\n",
    "                # exact root\n",
    "                return x, numpy.NaN, i, history\n",
    "    else:\n",
    "        raise RuntimeError(f\"Didn't converge after {maxit} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1.\n",
    "xstar, residual, it, history = newton(g1, dg1, x0, alpha=0.1, tol=1e-10, maxit=1000, history=[])\n",
    "xstar, g1(xstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.semilogy(numpy.abs(history - history[-1]), \".-\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "If the initial guess is not \"close\" in some sense to the root, then Newton's method only converges linearly (it is only near the root that quadratic convergence kicks in).\n",
    "\n",
    "Notice how we didn't converge to the *same* root as gradient descent, even with the same initial guess. If gradient descent starts out in a valley of the function we're trying to minimise, it will never get out of it; Newton might."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.linspace(-10, 10, 200)\n",
    "pyplot.figure()\n",
    "pyplot.plot(x, g1(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding global minima\n",
    "\n",
    "In the general case, finding the global minimum of a high-dimensional function is an unsolvable problem. The best we can do is to try and find multiple local minima and hope that we found one that was \"good enough\". There are various techniques that are constructed on a somewhat more or less ad-hoc basis to do this.\n",
    "\n",
    "If the function we're trying to minimise is [convex](https://en.wikipedia.org/wiki/Convex_function) then there are guarantees on existance, and uniqueness. Unfortunately, nature is not usually so kind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Accelerated\" gradient methods\n",
    "\n",
    "In the ML community, computing the second derivative of the function to be minimised is generally computationally too expensive. While for a problem with $n$ parameters the gradient is a vector in $\\mathbb{R}^n$, the second derivative is represented by a matrix in $\\mathbb{R}^{n \\times n}$. For mesh-based discretisations of PDEs this matrix is typically *a priori* sparse (as we have seen). In ML it may well not be.\n",
    "\n",
    "Hence, there is quite a lot of interest in coming up with extensions of gradient descent that speed up convergence (especially near minima), hopefully attaining quadratic convergence in the best case.\n",
    "\n",
    "One such example is *Nesterov acceleration*. At the cost of maintaining a little extra state, we can improve convergence in some cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov(g, x, alpha=1, mu=0.8, tol=1e-5, maxit=100, history=None):\n",
    "    yi = x\n",
    "    v = 0\n",
    "    for i in range(maxit):\n",
    "        gx = g(x)\n",
    "        if numpy.abs(gx) < tol:\n",
    "            return x, gx, i, history\n",
    "        # Standard gradient descent\n",
    "        v = mu*v - alpha*gx\n",
    "        x = x + mu*v - alpha*gx\n",
    "        if history is not None:\n",
    "            history.append(x)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Didn't converge in {maxit} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x0 = 1.\n",
    "xstar, residual, its, history = nesterov(g1, x0, alpha=0.5, mu=0.15, tol=1e-10, maxit=1000, history=[])\n",
    "xstar, g1(xstar), its"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.semilogy(numpy.abs(numpy.asarray(history) - history[-1]), \".-\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1.\n",
    "xstar, residual, its, history = gradient_descent(g1, x0, alpha=0.5, tol=1e-10, maxit=1000, history=[])\n",
    "xstar, g0(xstar), its"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.semilogy(numpy.abs(numpy.asarray(history) - history[-1]), \".-\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some 2D problems\n",
    "\n",
    "Let's look at a harder problem, that gradient descent really struggles with, but Newton works well.\n",
    "\n",
    "This problem is very smooth but it has a very flat \"valley\" with a minimum in it. Near the minimum, therefore, gradient descent really slows down, while far away we need a good damping parameter to not diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "def f(x, y):\n",
    "    return (1 - x)**2 + 100*(y-x**2)**2\n",
    "\n",
    "x = numpy.linspace(-2, 2, 100)\n",
    "x, y = numpy.meshgrid(x, x)\n",
    "pyplot.figure()\n",
    "ax = pyplot.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(x, y, f(x, y), cmap=\"viridis\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For gradient descent\n",
    "def g(X):\n",
    "    x, y = X\n",
    "    return numpy.asarray((2*(200*x**3 - 200*x*y + x - 1), 200*(y - x**2)))\n",
    "\n",
    "# For newton\n",
    "def dg(X):\n",
    "    x, y = X\n",
    "    return numpy.asarray([[1200*x**2 - 400*y + 2, -400*x],\n",
    "                          [-400*x, 200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = numpy.asarray([0.999, 0.999], dtype=float)\n",
    "\n",
    "xstar, gx, its, history = gradient_descent(g, x0, 0.0025, tol=1e-5, maxit=10000, history=[])\n",
    "xstar, f(*xstar), gx, its"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "x = numpy.linspace(-2, 2, 100)\n",
    "x, y = numpy.meshgrid(x, x)\n",
    "pyplot.contourf(x, y, f(x, y), levels=20)\n",
    "pyplot.colorbar()\n",
    "pyplot.plot(*zip(*history), \".-\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = numpy.asarray([0.25, 0.25], dtype=float)\n",
    "\n",
    "history = []\n",
    "xstar, gx, its, history = newton(g, dg, x0, alpha=1, tol=1e-10, maxit=1000, history=history)\n",
    "xstar, f(*xstar), its"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "x = numpy.linspace(-2, 2, 100)\n",
    "x, y = numpy.meshgrid(x, x)\n",
    "pyplot.contourf(x, y, f(x, y), levels=20)\n",
    "pyplot.colorbar()\n",
    "pyplot.plot(*zip(*history), \".-\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(1.001, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
