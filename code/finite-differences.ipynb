{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial discretisation\n",
    "\n",
    "So far, we've seen time derivatives and ordinary differential equations of the form\n",
    "\n",
    "$$\n",
    "\\dot{u} = f(t, u).\n",
    "$$\n",
    "\n",
    "Most problems one encounters in the real world have spatial as well as time derivatives. Our first example is the [*Poisson equation*](https://en.wikipedia.org/wiki/Poisson%27s_equation):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\frac{\\text{d}^2 u}{\\text{d} x^2} &= f(x) \\quad x \\in \\Omega = (-1, 1)\\\\\n",
    " u(-1) &= a\\\\\n",
    " \\frac{\\text{d} u}{\\text{d} x}(1) &= b\\\\\n",
    " \\end{align}.\n",
    "$$\n",
    "\n",
    "This is termed a *boundary value problem* (BVP), as opposed to the ODEs which were *initial value problems*, since we do not specify an initial condition, but rather a condition on the boundary of the domain.\n",
    "\n",
    "This equation appears in a remarkably large number of places. As one example, it models the equilibrium temperature profile of a thermally conducting material maintained at constant temperature $a$ on the left and cooled at constant rate $b$ on the right.\n",
    "\n",
    "To solve this problem numerically we must make a number of choices:\n",
    "\n",
    "- how to represent the solution $u$;\n",
    "- how to compute its derivatives;\n",
    "- how to enforce the boundary conditions;\n",
    "- how and where to evaluate $f$;\n",
    "- in what sense we would like our solution to satisfy the equation.\n",
    "\n",
    "[Iserles' book](http://www.damtp.cam.ac.uk/user/ai/Arieh_Iserles/Textbook.html) contains a quite mathematical treatment of finite differences. I also like [Randy LeVeque's *Finite difference methods for ordinary and partial differential equations*](http://staff.washington.edu/rjl/fdmbook/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference framework\n",
    "\n",
    "We will focus on _finite difference_ methods here, which make the following choices in answer to the questions above:\n",
    "\n",
    "- The solution $u(x)$ is represented by _pointwise_ values $u_i = u(x_i)$ at some discrete set of points $-1 = x_0 < x_1 < \\dots < x_N = 1$. Importantly, the framework _does not_ specify the value of $u$ outside of these points;\n",
    "- derivatives of $u$ at points $x_i$ are approximated using differencing formulae that utilise a finite number of neighbouring points (independent of $N$);\n",
    "- boundary conditions are either enforced pointwise (e.g. the $u(-1)$ case above), or (when constraining derivatives) with one-sided differencing formulae;\n",
    "- $f$ is evaluated pointwise at each $x_i$;\n",
    "- we require that our finite difference method satisfies the equation pointwise at each $x_i$ in the interior of the domain.\n",
    "\n",
    "### Differencing formulae\n",
    "\n",
    "Our starting point is the definition of a derivative:\n",
    "\n",
    "$$\n",
    "\\frac{d u(x)}{d x} = \\lim_{\\epsilon \\to 0} \\frac{u(x + \\epsilon) - u(x)}{\\epsilon}.\n",
    "$$\n",
    "\n",
    "If we wish to approximate this in our finite difference framework, where we only have point values, we can do so using neighbouring values. Writing $x_{i+1} - x_i = h$ for convenience, we can write\n",
    "\n",
    "$$\n",
    "\\frac{d u(x_i)}{d x} \\approx \\frac{u(x_{i+1}) - u(x_i)}{h} = \\frac{u_{i+1} - u_i}{h} =: D_+ u_i.\n",
    "$$\n",
    "\n",
    "This is a _one-sided_ approximation: we only use $u_i$ and $u_{i+1}$. Another one-sided approximation would be to offset in the other direction\n",
    "\n",
    "$$\n",
    "D_{-} u_i := \\frac{u_i - u_{i-1}}{h}.\n",
    "$$\n",
    "\n",
    "Finally, we can also use a _centred_ approximation by averaging the two one-sided approximations:\n",
    "\n",
    "$$\n",
    "D_0 u_i := \\frac{u_{i+1} - u_{i-1}}{2h} = \\frac{1}{2} (D_+ + D_-) u_i.\n",
    "$$\n",
    "\n",
    "Let's have a picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.lines as mlines\n",
    "pyplot.style.use('ggplot')\n",
    "\n",
    "n = 200\n",
    "h = 2/(n-1)\n",
    "x = numpy.linspace(1,2.5,n)\n",
    "pyplot.plot(x, numpy.sin(x));\n",
    "\n",
    "def newline(p1, p2, **kwargs):\n",
    "    ax = pyplot.gca()\n",
    "    xmin, xmax = ax.get_xbound()\n",
    "\n",
    "    if(p2[0] == p1[0]):\n",
    "        xmin = xmax = p1[0]\n",
    "        ymin, ymax = ax.get_ybound()\n",
    "    else:\n",
    "        ymax = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmax-p1[0])\n",
    "        ymin = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmin-p1[0])\n",
    "\n",
    "    l = mlines.Line2D([xmin,xmax], [ymin,ymax], **kwargs)\n",
    "    ax.add_line(l)\n",
    "    return l\n",
    "\n",
    "h = 0.25\n",
    "xi = 1.6\n",
    "ximinus = xi - h\n",
    "xiplus = xi + h\n",
    "\n",
    "pyplot.plot([ximinus, xi, xiplus], numpy.sin([ximinus, xi, xiplus]), marker=\"o\", linestyle=\"none\")\n",
    "\n",
    "newline((xi, numpy.sin(xi)), (xiplus, numpy.sin(xiplus)), linestyle=\"dashed\", label=\"$D_+ u(x)$\")\n",
    "newline((xi, numpy.sin(xi)), (ximinus, numpy.sin(ximinus)), linestyle=\"dotted\", label=\"$D_- u(x)$\")\n",
    "newline((ximinus, numpy.sin(ximinus)), (xiplus, numpy.sin(xiplus)), linestyle=\"-.\", label=\"$D_0 u(x)$\")\n",
    "\n",
    "\n",
    "newline((xi, numpy.sin(xi)), (xiplus, numpy.sin(xi) + h*numpy.cos(xi)), color=\"black\", label=\"$u'(x)$\")\n",
    "\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the full approximate derivative too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "h = 2/(n-1)\n",
    "x = numpy.linspace(-1,1,n)\n",
    "u = numpy.sin(x)\n",
    "pyplot.figure()\n",
    "pyplot.plot(x, numpy.cos(x), label=\"$u'$\");\n",
    "pyplot.plot(x[1:], (u[1:] - u[:-1])/h, label=\"$D_-$\", marker=\"o\", linestyle=\"none\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Certainly from the picture of the slope above, it appears that the centered difference formula is more accurate than the one-sided approximations. Can we formalise this at all?\n",
    "\n",
    "To do so, we turn to the favourite tool of the budding numericist, the *taylor expansion*.\n",
    "\n",
    "### Recap, Taylor expansions\n",
    "\n",
    "For a sufficiently smooth function $u$, given a point $x$, we can represent the function at a new point $x + h$ by its Taylor expansion\n",
    "\n",
    "$$\n",
    "u(x + h) = u(x) + u'(x) h + \\frac{1}{2} u''(x) h^2 + \\frac{1}{6} u'''(x) h^3 + \\dots = \\sum_{n=0}^\\infty \\frac{1}{n!} h^n u^{(n)}(x) \n",
    "$$\n",
    "\n",
    "where the notation $u'$ is shorthand for $\\frac{\\text{d} u}{\\text{d} x}$ and $u^{(n)}(x) = \\frac{\\text{d}^n u}{\\text{d} x^n}$.\n",
    "\n",
    "Let's look at how this works for a sample function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "def u(x, n=0):\n",
    "    factor = (-1)**(n // 2)\n",
    "    if n % 2 == 0:\n",
    "        u_ = numpy.sin\n",
    "    else:\n",
    "        u_ = numpy.cos\n",
    "    return factor * u_(x)\n",
    "\n",
    "pyplot.figure()\n",
    "\n",
    "x = numpy.linspace(0.5, 1.75, 500)\n",
    "\n",
    "pyplot.plot(x, u(x));\n",
    "\n",
    "x0 = 0.6\n",
    "h = 0.8\n",
    "\n",
    "def fac(n):\n",
    "    return reduce(mul, range(1, n+1), 1)\n",
    "\n",
    "def taylor(u, x0, h, n):\n",
    "    return u(x0) + sum(h**i/fac(i) * u(x0, i) for i in range(1, n))\n",
    "\n",
    "xs = numpy.linspace(x0, x0+h, 20)\n",
    "for n in range(1, 5):\n",
    "    pyplot.plot(xs, [taylor(u, x0, x - x0, n) for x in xs], marker=\"o\", label=r\"$\\tilde{u} + \\mathcal{O}(h^%d)$\" % n)\n",
    "\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we chop off the series at some finite $n$ we write\n",
    "\n",
    "$$\n",
    "u(x + h) = u(x) + u'(x) h + \\frac{1}{2} u''(x) h^2 + \\mathcal{O}(h^3)\n",
    "$$\n",
    "\n",
    "with $h$ sufficiently small.\n",
    "\n",
    "To determine the order of a method, we substitute the Taylor expansion into the differencing expression and calculate.\n",
    "\n",
    "As an example, let us consider the one-sided differencing operator $D_+$. To simplify notation, we will choose $x = 0$, and we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u'(0) &\\approx \\frac{u(h) - u(0)}{h} \\quad \\text{ definition of } D_+\\\\\n",
    "      &= h^{-1}(\\underbrace{u(0) + u'(0) h + \\frac{1}{2} u''(0) h^2 + \\mathcal{O}(h^3)}_{u(h)} - u(0)) \\\\\n",
    "      &= u'(0) + \\frac{1}{2} u''(0) h + \\mathcal{O}(h^2)\\\\\n",
    "\\end{align}.\n",
    "$$\n",
    "\n",
    "The leading-order error term in the right hand side is $\\mathcal{O}(h)$, and so we say that this is a _first-order_ method. Derivation that the operator $D_-$ is also first-order proceeds identically.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "1. Show that the centered difference operator $D_0$ computes a second-order accurate derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability\n",
    "\n",
    "We will postpone mathematical discussion of stability for a while, and give an intuition for some potential problems. Let us first check that our implementation of differencing operators provides us with the expected (mathematical) convergence orders for a smooth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dplus(x, u):\n",
    "    return x[:-1], (u[1:] - u[:-1])/(x[1:] - x[:-1])\n",
    "\n",
    "def dminus(x, u):\n",
    "    return x[1:], (u[1:] - u[:-1])/(x[1:] - x[:-1])\n",
    "\n",
    "def center(x, u):\n",
    "    return x[1:-1], (u[2:] - u[:-2])/(x[2:] - x[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = 2**numpy.arange(3, 10)\n",
    "\n",
    "def error(f, df, op):\n",
    "    for n in grids:\n",
    "        x = numpy.linspace(-1, 1, n)\n",
    "        x, y = op(x, f(x))\n",
    "        yield numpy.linalg.norm(y - df(x), numpy.inf)\n",
    "\n",
    "pyplot.figure()\n",
    "for op in [dplus, dminus, center]:\n",
    "    pyplot.loglog(1/grids, list(error(numpy.sin, numpy.cos, op)), marker=\"o\", linestyle=\"none\", label=op.__name__)\n",
    "    \n",
    "pyplot.xlabel(\"Resolution ($h$)\")\n",
    "pyplot.ylabel(\"$l_\\infty$ error in derivative\")\n",
    "\n",
    "pyplot.loglog(1/grids, 1/grids, label=\"$h$\")\n",
    "pyplot.loglog(1/grids, 1/grids**2, label=\"$h^2$\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So both the one-sided differences are first-order accurate, whereas the centered difference is second-order accurate, as expected. One thing to be wary of, however, is using some of these approximations for functions that are \"rough\" on the grid scale.\n",
    "\n",
    "We can make this question more precise by asking whether there are functions whose derivatives are non-zero, but for which our numerical approximations compute $u'(x_i) = 0$.\n",
    "\n",
    "Let's try and contrive an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.linspace(-1, 1, 9)\n",
    "xf = numpy.linspace(-1, 1, 100)\n",
    "\n",
    "def f(x):\n",
    "    return numpy.cos(1/2 + 4*numpy.pi*x)\n",
    "\n",
    "def df(x):\n",
    "    return -4*numpy.pi*numpy.sin(1/2 + 4*numpy.pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(x, f(x), marker=\"o\", label=\"coarse\")\n",
    "pyplot.plot(xf, f(xf), \"-\", label=\"fine\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the derivatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "for op in [dplus, dminus, center]:\n",
    "    x_, y = op(x, f(x))\n",
    "    pyplot.plot(x_, y, \"o-\", label=op.__name__)\n",
    "pyplot.plot(xf, df(xf), \"-\", label=\"Exact\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centered difference approximation produces a _zero_ derivative for this function. Hence if we have a solution $u(x)$, we can (at least to the numerical operator) construct a new solution $\\tilde{u}(x) = u(x) + f(x)$.\n",
    "\n",
    "Suddenly, even if our actual equation has a unique solution, the numerical solution is no longer unique. This turns out to cause all kinds of terrible issues with numerical algorithms and must be avoided at all costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-order derivatives\n",
    "\n",
    "We can compute high-order derivatives by repeatedly applying differencing operators for lower-order derivatives.\n",
    "\n",
    "For example, the second derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\text{d}^2 u}{\\text{d} x^2} \\approx D^2 u_i = D_+ D_- u_i = \\frac{1}{h^2}\\left(u_{i+1} - 2 u_i + u_{i-1}\\right) = D_- D_+ u_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Show that this is a _second-order_ accurate approximation of the second derivative\n",
    "2. We could also use $D^2 u_i = D_0 D_0 u_i$, derive the stencil for this case.\n",
    "3. Finally, show that if we define a \"half-step\" centered difference operator\n",
    "\n",
    "$$\n",
    "\\hat{D}_0 u = \\frac{1}{h}\\left[u\\left(x + \\frac{h}{2}\\right) - u\\left(x - \\frac{h}{2}\\right)\\right]\n",
    "$$\n",
    "then we have\n",
    "\n",
    "$$\n",
    "D^2 = \\hat{D}_0 \\circ \\hat{D}_0 = D_+ \\circ D_-\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary conditions\n",
    "\n",
    "The final missing piece required before we can solve the our first PDE is to figure out how we will treat boundary conditions. To do this, we will first recast the differencing operators as matrices. it is then somewhat easier to see what is going on.\n",
    "\n",
    "We can think of the differencing operator acting on an entire vector\n",
    "\n",
    "$$\n",
    "U = \\begin{bmatrix}\n",
    "u_0\\\\\n",
    "u_1\\\\\n",
    "\\vdots\\\\\n",
    "u_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "at once. For example, we can write\n",
    "\n",
    "$$\n",
    "D_+ = \\frac{1}{h} \\begin{bmatrix}\n",
    "-1 & 1 & 0 & \\dots & 0\\\\\n",
    "0 & -1 & 1 & \\dots & 0\\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots & \\vdots\\\\\n",
    "0 & \\dots & 0 & -1 & 1\\\\\n",
    "0 & \\dots & 0 & 0 & -1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "D^2 = \\frac{1}{h^2} \\begin{bmatrix}\n",
    "-2 & 1 & 0 &  \\dots & 0\\\\\n",
    "1 & -2 & 1 &  \\dots & 0\\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots & \\vdots\\\\\n",
    "0 & \\dots & \\dots 1 & -2 & 1\\\\\n",
    "0 & \\dots & 0 & 1 & -2\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Recall, our problem was to find $u \\in (-1, 1)$ satisfying\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\text{d}^2 u}{\\text{d} x^2} u &= f\\\\\n",
    "u(-1) &= a\\\\\n",
    "\\frac{\\text{d} u}{\\text{d} x}(1) &= b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In matrix form, this becomes\n",
    "\n",
    "$$\n",
    "\\underbrace{D^2}_{A} \\underbrace{\\begin{bmatrix}\n",
    "u_0\\\\\n",
    "u_1\\\\\n",
    "\\vdots\\\\\n",
    "u_N\n",
    "\\end{bmatrix}}_{U} = \\underbrace{\\begin{bmatrix} f_0\\\\f_1\\\\\\vdots\\\\f_N\\end{bmatrix}}_{F}.\n",
    "$$\n",
    "\n",
    "This works perfectly in the interior of the domain, but we need to figure out what to do at the boundaries. For example, we can't use the standard differencing operator for $D^2$ on $u_N$, because $u_{N+1}$ does not exist.\n",
    "Fortunately, our boundary conditions inform how to modify the matrix appropriately.\n",
    "\n",
    "### Dirichlet conditions\n",
    "\n",
    "These conditions, of the form \n",
    "\n",
    "$$\n",
    "u(-1) = a\n",
    "$$\n",
    "\n",
    "specify the _value_ of the solution at a particular point (or set of points). This means, that rather than solving a small equation to determine the value at this point, we _already know_ and can instead replace the relevant rows of the matrix. Let us suppose we have ordered our points such that $u(-1)$ corresponds to $u_0$. Then we have\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & \\dots & 0\\\\\n",
    " & & & \\\\\n",
    " & & A_{1:,:} & \\\\\n",
    " & & &\\\\\n",
    "\\end{bmatrix}\n",
    "U = \\begin{bmatrix} a\\\\ \\\\ F_{1:} \\\\ \\\\ \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "In general, if we have a boundary value $\\alpha_i$ that constrains $u_i$ then we replace the $i$th row with the identity, and the $i$th value in the right hand side with $\\alpha_i$.\n",
    "\n",
    "This modification destroys any symmetry that might have existed in the matrix $A$, since we have zeroed rows, but not the corresponding columns. If we write the linear system in block form, we can, however, see a way around this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "I & 0\\\\\n",
    "A_{10} & A_{11}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "U_0\\\\\n",
    "U_1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "F_0\\\\\n",
    "F_1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since we know $U_0$ (they are just $F_0$), we can forward-substitute and move the lower-left block of the matrix onto the right hand side, to produce\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "I & 0\\\\\n",
    "0 & A_{11}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "U_0\\\\\n",
    "U_1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "F_0\\\\\n",
    "F_1 - A_{10}F_0\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "This is often a convenient form to work with.\n",
    "\n",
    "Alternately, since the equations for $U_0$ are just the identity, we can write our solver to just handle\n",
    "\n",
    "$$\n",
    "A_{11} U_1 = F_1 - A_{10}F_0\n",
    "$$\n",
    "\n",
    "and insert the boundary values into a big vector whenever we need to visualise it.\n",
    "\n",
    "\n",
    "#### Special Case: Homogeneous Dirichlet conditions\n",
    "\n",
    "In the special case where all Dirichlet conditions are equal to zero (also referred to as homogeneous Dirichlet conditions), we have:\n",
    "\n",
    "$$\n",
    "A_{11} U_1 = F_1 - A_{10}F_0 = F_1,\n",
    "$$\n",
    "\n",
    "i.e. we can drop the boundary conditions completely since the entries in $F_0$ are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neumann conditions\n",
    "\n",
    "We can now treat boundary conditions that constrain the value of the solution, but recall that the condition at $x=1$ instead constrains gradient of the solution. We cannot do this by setting values, but must instead form an equation for the boundary value.\n",
    "\n",
    "### One-sided difference\n",
    "\n",
    "There are typically two ways to do this. We either come up with a one-sided differencing formula for the derivative directly. For example, recalling the one-sided difference we might replace the boundary term\n",
    "\n",
    "$$\n",
    "\\frac{\\text{d} u}{\\text{d} x}(1) = b\n",
    "$$\n",
    "\n",
    "by\n",
    "\n",
    "$$\n",
    "\\frac{u_n - u_{n-1}}{h} = b.\n",
    "$$\n",
    "\n",
    "This is simple but has some potential drawbacks\n",
    "\n",
    "1. We need to make a different choice for the discretisation on the boundary to that in the interior\n",
    "2. This choice may not have the same order of accuracy\n",
    "3. It may destroy symmetry that previously existed in the problem.\n",
    "\n",
    "### Ghost values\n",
    "\n",
    "An alternate option is to introduce a (or possibly more than one) _ghost value_ outside of the domain such that we can then just use our interior discretisation. We then define the value of this ghost point to be the reflection (possibly weighted by the boundary value) of the interior point.\n",
    "\n",
    "That is, we introduce $u_{n+1} = u(x_{n+1})$ and set\n",
    "\n",
    "$$\n",
    "u_{n+1} = u_{n-1} + 2b(\\underbrace{x_n - x_{n-1}}_{h}).\n",
    "$$\n",
    "\n",
    "Now we can use our interior discretisation\n",
    "\n",
    "$$\n",
    "\\frac{-u_{n-1} + 2u_n - u_{n+1}}{h^2} = f(x_n)\n",
    "$$\n",
    "\n",
    "substituting in the definition of $u_{n+1}$ we obtain\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{-u_{n-1} + 2u_n - (u_{n-1} + 2bh)}{h^2} &= f(x_n)\\\\\n",
    "\\frac{2(u_n - u_{n-1})}{h^2} &= f(x_n) + \\frac{2b}{h}\\\\\n",
    "\\frac{u_n - u_{n-1}}{h^2} &= \\frac{f(x_n)}{2} + \\frac{b}{h}\n",
    "\\end{align}.\n",
    "$$\n",
    "\n",
    "Let's compare these approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian(N, rhsfunc):\n",
    "    x = numpy.linspace(0, 1, N+1)\n",
    "    h = 1/N\n",
    "    rhs = rhsfunc(x)\n",
    "    e = numpy.ones(N)\n",
    "    # interior discretisation\n",
    "    L = (2*numpy.eye(N+1) - numpy.diag(e, 1) - numpy.diag(e, -1)) / h**2\n",
    "    return x, L, rhs, h\n",
    "    \n",
    "def apply_dirichlet(L, rhs, h, vals, indices):\n",
    "    N, _ = L.shape\n",
    "    diag = numpy.eye(1, N)\n",
    "    bcmask = numpy.zeros(N, dtype=bool)\n",
    "    bcmask[indices] = True\n",
    "    # Dirichlet rows\n",
    "    L[numpy.ix_(bcmask)] = numpy.vstack([numpy.roll(diag, i) for i in indices])\n",
    "    rhs[numpy.ix_(bcmask)] = vals\n",
    "    # Forward substitute\n",
    "    rhs[numpy.ix_(~bcmask)] -= L[numpy.ix_(~bcmask, bcmask)] @ vals\n",
    "    L[numpy.ix_(~bcmask, bcmask)] = 0\n",
    "    return L, rhs, h\n",
    "\n",
    "def apply_neumann_oneside(L, rhs, h, b, index):\n",
    "    N, _ = L.shape\n",
    "    assert index == N - 1\n",
    "    L[index, :] = 0\n",
    "    L[index, index] = 1/h\n",
    "    L[index, index - 1] = -1/h\n",
    "    rhs[index] = b\n",
    "    return L, rhs, h\n",
    "\n",
    "def apply_neumann_ghost(L, rhs, h, b, index):\n",
    "    N, _ = L.shape\n",
    "    L[index, index] /= 2\n",
    "    rhs[index] = b/h + rhs[index]/2\n",
    "    return L, rhs, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll solve\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\frac{\\text{d}^2 u}{\\text{d} x^2} &= e^x \\text{ in } (0, 1)\\\\\n",
    "u(0) &= e^{0}\\\\\n",
    "\\frac{\\text{d} u}{\\text{d} x} &= e^{1}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with convenient exact solution $u(x) = e^{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "rhsfunc = lambda x: -numpy.exp(x)\n",
    "exact = lambda x: numpy.exp(x)\n",
    "x, L, rhs, h = laplacian(N, rhsfunc)\n",
    "L, rhs, h = apply_dirichlet(L, rhs, h, [exact(0)], [0])\n",
    "L, rhs, h = apply_neumann_oneside(L, rhs, h, exact(1), N)\n",
    "uoneside = numpy.linalg.solve(L, rhs)\n",
    "\n",
    "x, L, rhs, h = laplacian(N, rhsfunc)\n",
    "L, rhs, h = apply_dirichlet(L, rhs, h, [exact(0)], [0])\n",
    "L, rhs, h = apply_neumann_ghost(L, rhs, h, exact(1), N)\n",
    "ughost = numpy.linalg.solve(L, rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(x, uoneside, label=\"Computed Oneside\")\n",
    "pyplot.plot(x, ughost, label=\"Computed Ghost\")\n",
    "pyplot.plot(x, exact(x), label=\"Exact\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Perhaps unsurprisingly, the one-sided application of the Neumann conditions performs worse than the ghost version. Interestingly, they are effectively the *same* discretisation in the matrix, the only difference is that in the ghost version, we corrected the right hand side we're solving for by a small amount to take into account the issues in the one-sided discretisation.\n",
    "\n",
    "It looks like we have a lower-order scheme. Let's check by performing an MMS test.\n",
    "\n",
    "To do so, we have to introduce how to measure errors. Since our discrete solution $u_i$ is supposed to approximate $u(x_i)$ it is natural to consider the pointwise errors $u_i - u(x_i)$. Let us now consider how to measure the size of the error vector (or indeed any vector).\n",
    "\n",
    "$$\n",
    "E = \\begin{bmatrix} u_0 - u(x_0)\\\\\n",
    "\\vdots\\\\\n",
    "u_n - u(x_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So far we have been using the $\\infty$-norm or $\\max$-norm.\n",
    "\n",
    "$$\n",
    "\\|E\\|_\\infty := \\max_{0 \\le i \\le n} |E_i| = \\max_{0 \\le i \\le n} |u_i - u(x_i)|\n",
    "$$\n",
    "\n",
    "which measures the largest pointwise error over the interval.\n",
    "\n",
    "Other common norms are the $1$-norm\n",
    "\n",
    "$$\n",
    "\\|E\\|_1 = h \\sum_{i=0}^n |E_i|\n",
    "$$\n",
    "\n",
    "and the $2$-norm\n",
    "\n",
    "$$\n",
    "\\|E\\|_2 = \\left(h \\sum_{i=0}^n |E_i|^2 \\right)^{1/2}.\n",
    "$$\n",
    "\n",
    "Notice the factor of $h$ appearing in these definitions. This is needed so the norm does not spuriously grow when we add more points.\n",
    "\n",
    "### Aside\n",
    "\n",
    "These are special cases of $l_p$ norms\n",
    "\n",
    "$$\n",
    "\\|E\\|_p = \\left(h \\sum_{i=0}^n |E_i|^p\\right)^{1/p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the 2-norm\n",
    "def error(u, exact, h):\n",
    "    return numpy.sqrt(h)*numpy.linalg.norm(u - exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mms_errors(neumann):\n",
    "    errors = []\n",
    "    Ns = numpy.asarray(list(2**i for i in range(4, 11)))\n",
    "    rhsfunc = lambda x: -numpy.exp(x)\n",
    "    exact = lambda x: numpy.exp(x)\n",
    "    for N in Ns:\n",
    "        x, L, rhs, h = laplacian(N, rhsfunc)\n",
    "        L, rhs, h = apply_dirichlet(L, rhs, h, [exact(0)], [0])\n",
    "        L, rhs, h = neumann(L, rhs, h, exact(1), N)\n",
    "        u = numpy.linalg.solve(L, rhs)\n",
    "        errors.append(error(u, exact(x), 1/N))\n",
    "    return 1/Ns, numpy.asarray(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, oneside = mms_errors(apply_neumann_oneside)\n",
    "hs, ghost = mms_errors(apply_neumann_ghost)\n",
    "pyplot.figure()\n",
    "pyplot.loglog(hs, oneside, \"o\", label=\"Oneside\");\n",
    "pyplot.loglog(hs, ghost, \"x\", label=\"Ghost\");\n",
    "pyplot.loglog(hs, hs, label=\"$\\mathcal{O}(h)$\");\n",
    "pyplot.loglog(hs, hs**2, label=\"$\\mathcal{O}(h^2)$\");\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our suspicion that the one-sided differencing for the Neumann condition is only first-order accurate.\n",
    "\n",
    "An alternative approach to obtaining a second-order scheme (rather than the ghost method above) is to try and determine a second-order accurate one-sided difference approximation to the first derivative. We will state an example first, and then see where it comes from.\n",
    "\n",
    "A second-order accurate one-sided approximation to the first derivative is obtained with\n",
    "\n",
    "$$\n",
    "\\frac{\\text{d} u}{\\text{d} x} \\approx \\frac{1}{h}\\left(\\frac{3}{2} u_i - 2 u_{i-1} + \\frac{1}{2} u_{i-2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_neumann_oneside_second(L, rhs, h, b, index):\n",
    "    N, _ = L.shape\n",
    "    assert index == N - 1\n",
    "    L[index, :] = 0\n",
    "    L[index, index] = 3/(2*h)\n",
    "    L[index, index - 1] = -2/h\n",
    "    L[index, index - 2] = 1/(2*h)\n",
    "    rhs[index] = b\n",
    "    return L, rhs, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, oneside = mms_errors(apply_neumann_oneside)\n",
    "_, second = mms_errors(apply_neumann_oneside_second)\n",
    "hs, ghost = mms_errors(apply_neumann_ghost)\n",
    "pyplot.figure()\n",
    "pyplot.loglog(hs, oneside, \"o\", label=\"Oneside\");\n",
    "pyplot.loglog(hs, second, \"s\", label=\"Oneside second order\");\n",
    "pyplot.loglog(hs, ghost, \"x\", label=\"Ghost\");\n",
    "pyplot.loglog(hs, hs, label=\"$\\mathcal{O}(h)$\");\n",
    "pyplot.loglog(hs, hs**2, label=\"$\\mathcal{O}(h^2)$\");\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This converges at second order as, perhaps, expected. The absolute error is a little worse than the ghosted version. However, this approach is sometimes more convenient, especially on irregularly spaced meshes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving high-order finite difference stencils\n",
    "\n",
    "Where did the approximation\n",
    "\n",
    "$$\n",
    "\\frac{\\text{d} u}{\\text{d} x} \\approx \\frac{1}{h}\\left(\\frac{3}{2} u_i - 2 u_{i-1} + \\frac{1}{2} u_{i-2}\\right)\n",
    "$$\n",
    "\n",
    "come from? Given some points at which we're allowed to evaluate $u$, we can derive an appropriate formula from the Taylor series using the *method of undetermined coefficients*. This works in a very similar way to determining the truncation error for a given expansion.\n",
    "\n",
    "For the example above, we want to approximation $u'(x)$ and we are given $u_i = u(x)$, $u_{i-1} = u(x - h)$, and $u_{i-2} = u(x - 2h)$. We can write our differencing operator as a *linear combination* of the provided points\n",
    "\n",
    "$$\n",
    "D_2 u(x) = a u(x) + bu(x-h) + c u(x - 2h)\n",
    "$$\n",
    "\n",
    "where our goal is to determine $a$, $b$, and $c$ to minimise the truncation error (that is, give the best possible accuracy).\n",
    "\n",
    "Let's Taylor-expand on the right hand side\n",
    "\n",
    "$$\n",
    "D_2 u(x) = a u(x) + b \\overbrace{\\left(u(x) - hu'(x) + \\frac{h^2}{2} u''(x) - \\frac{h^3}{6} u'''(x)\\right)}^{u(x - h)} + c \\overbrace{\\left(u(x) - 2h u'(x) + \\frac{4h^2}{2}u''(x) - \\frac{8 h^3}{6} u'''(x)\\right)}^{u(x-2h)} + \\mathcal{O}(h^4)\n",
    "$$\n",
    "\n",
    "gathering terms we have\n",
    "\n",
    "$$\n",
    "D_2 u(x) = (a + b + c)u(x) - (b + 2c) h u'(x) + \\frac{1}{2}(b + 4c)h^2 u''(x) - \\frac{1}{6}(b + 8c) h^3 u'''(x) + \\mathcal{O}(h^4).\n",
    "$$\n",
    "\n",
    "To maximise the accuracy of agreement with $u'(x)$ we need\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a + b + c &= 0 && \\text{zeroing the $h^0 u(x)$ term}\\\\\n",
    "b + 2c &= -\\frac{1}{h} && \\text{ensuring that we have a $u'(x)$ term}\\\\\n",
    "b + 4c &= 0 && \\text{zeroing the $h^2 u''(x)$ term}\\\\ \n",
    "b + 8c &= 0 && \\text{zeroing the $h^3 u'''(x)$ term}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since we have only three unknowns, we can only satisfy three equations. To maxmimise the accuracy, we'll choose to zero the $h^2$ and $h^0$ terms, and live with the $h^3$ term. We therefore need to solve the linear system\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "0 & 1 & 2\\\\\n",
    "0 & 1 & 4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a \\\\ b \\\\ c\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "0 \\\\ -\\frac{1}{h} \\\\ 0\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "A = numpy.asarray([[1, 1, 1],\n",
    "                   [0, 1, 2],\n",
    "                   [0, 1, 4]])\n",
    "b = numpy.asarray([0, -1, 0])\n",
    "\n",
    "numpy.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a \\\\ b \\\\ c\n",
    "\\end{bmatrix} =\n",
    "\\frac{1}{2h}\n",
    "\\begin{bmatrix}\n",
    "3\\\\\n",
    "-4\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and hence our optimal formula is\n",
    "\n",
    "$$\n",
    "D_2 u(x) = \\frac{1}{2h}(3 u_i - 4 u_{i-1} + u_{i-2})\n",
    "$$\n",
    "\n",
    "as advertised. We can immediately determine the accuracy of this approximation since we know the first term we did not manage to match exactly is\n",
    "\n",
    "$$\n",
    "- \\frac{1}{6}(b + 8c) h^3 u'''(x)\n",
    "$$\n",
    "\n",
    "substituting in the values for $b$ and $c$ we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_2 u(x) - u'(x) &= -\\frac{1}{6}\\left(\\frac{-2}{h} + \\frac{8}{2h}\\right) h^3 u'''(x) + \\mathcal{O}(h^4)\\\\\n",
    "                 &= -\\frac{1}{3} h^2 u'''(x) + \\mathcal{O}(h^4)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "so this approximation is second order accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtwo(x, u):\n",
    "    h = x[2:] - x[1:-1]\n",
    "    du = 1/(2*h) * (3 * u[2:] - 4*u[1:-1] + u[:-2])\n",
    "    return x[2:], du\n",
    "\n",
    "grids = 2**numpy.arange(3, 10)\n",
    "\n",
    "def error(f, df, op):\n",
    "    for n in grids:\n",
    "        x = numpy.linspace(-1, 1, n)\n",
    "        x, y = op(x, f(x))\n",
    "        yield numpy.sqrt(1/n)*numpy.linalg.norm(y - df(x), None)\n",
    "\n",
    "pyplot.figure()\n",
    "pyplot.loglog(1/grids, list(error(numpy.sin, numpy.cos, dtwo)), marker=\"o\", linestyle=\"none\", label=dtwo.__name__)\n",
    "    \n",
    "pyplot.xlabel(\"Resolution ($h$)\")\n",
    "pyplot.ylabel(\"$l_2$ error in derivative\")\n",
    "\n",
    "pyplot.loglog(1/grids, 1/grids**2, label=\"$h^2$\")\n",
    "pyplot.loglog(1/grids, 1/grids**2, label=\"$h^2$\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.linalg.norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Use this technique to derive a third-order accurate one-sided differencing operator for $\\frac{\\text{d}}{\\text{d} x}$ using 4 points $u(x)$, $u(x - h)$, $u(x - 2h)$, $u(x - 3h)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differencing for advection\n",
    "\n",
    "For centered difference approximations, there is no asymmetry in the stencil. For one-sided approximations, however, there is. If we just consider a three-point region centered around a point $i$, we can write the three approximations to $\\frac{\\text{d}}{\\text{d} x}$ we have seen as stencils:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_+ &= \\frac{1}{h}\\begin{bmatrix}0 & -1 &1\\end{bmatrix}\\\\\n",
    "D_- &= \\frac{1}{h}\\begin{bmatrix}-1 &  1 & 0\\end{bmatrix}\\\\\n",
    "D_0 &= \\frac{1}{2h}\\begin{bmatrix}-1 & 0 & 1\\end{bmatrix}\\\\\n",
    "\\end{aligned}.\n",
    "$$\n",
    "\n",
    "Recall that we noticed that the centered difference approximation sometimes gave catastrophic results (all zero derivatives) for very rough functions. The question therefore might arise how to pick between $D_+$ and $D_-$. We will study this using the linear *advection equation* as a prototype.\n",
    "\n",
    "This equation models the transport of some material by a bulk motion. This (especially when talking about fluid flow) is called convection. As usual [wikipedia has lots of information](https://en.wikipedia.org/wiki/Advection).\n",
    "\n",
    "It looks remarkably benign, find $u(x, t)$ satisfying\n",
    "\n",
    "$$\n",
    "\\partial_t u + c \\cdot \\nabla u = f(t, x)\n",
    "$$\n",
    "\n",
    "Where $c$ is the advecting velocity and\n",
    "\n",
    "$$\n",
    "\\nabla u = \\partial_x u\n",
    "$$\n",
    "in one dimension, and\n",
    "$$\n",
    "\\nabla u = \\begin{bmatrix} \\partial_x u\\\\ \\partial_y u\\end{bmatrix}\n",
    "$$\n",
    "in two dimensions.\n",
    "\n",
    "This is a first-order PDE, for which we need to supply one boundary condition (to pin down the spatial derivative) and one initial condition (to start everything off). The boundary condition, it turns out, has to be at the *inflow* boundary.\n",
    "\n",
    "Let's try and solve this equation with an explicit Euler time integration scheme on the interval $[0, 5]$ and look at the effect of the different differencing operators. If we write this out we have (using superscripts for time points and subscripts for spatial points)\n",
    "\n",
    "$$\n",
    "u^{n+1}_i = u^n_i + \\Delta t (f^n_i - c D u^n_i).\n",
    "$$\n",
    "\n",
    "We'll set the forcing function $f$ to be zero and pick boundary condition\n",
    "\n",
    "$$\n",
    "u(t, 0) = 0\n",
    "$$\n",
    "\n",
    "and initial condition\n",
    "\n",
    "$$\n",
    "u(0, x) = e^{-2(x - 2.5)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 30\n",
    "nx = 50\n",
    "x = numpy.linspace(0, L, nx + 1)\n",
    "h = L/nx\n",
    "u = numpy.exp(-2*(x - L/2)**2)\n",
    "uhat = numpy.zeros_like(x)\n",
    "uhat[nx//3:2*nx//3] = 1\n",
    "\n",
    "\n",
    "def Aupwind(nx, h, c):\n",
    "    A = numpy.zeros((nx+1, nx+1), dtype=float)\n",
    "    for i in range(1, nx+1):\n",
    "        A[i, i-1] = c/h\n",
    "        A[i, i] = -c/h\n",
    "    # Boundary condition (will be fixed later)\n",
    "    A[0, 0] = 0\n",
    "    return A\n",
    "\n",
    "def Adownwind(nx, h, c):\n",
    "    A = numpy.zeros((nx+1, nx+1), dtype=float)\n",
    "    for i in range(1, nx):\n",
    "        A[i, i] = c/h\n",
    "        A[i, i+1] = -c/h\n",
    "    # Boundary condition (will be fixed later)\n",
    "    A[0, 0] = 0\n",
    "    A[nx, nx] = c/h\n",
    "    return A\n",
    "\n",
    "t = 0\n",
    "tfinal = 10\n",
    "c = 1\n",
    "dt = 2/(nx + 1)\n",
    "\n",
    "Id = numpy.eye(nx+1)\n",
    "\n",
    "downwind = Id + dt*Adownwind(nx, h, c)\n",
    "upwind = Id + dt*Aupwind(nx, h, c)\n",
    "\n",
    "u = uhat\n",
    "hist = [(t, u)]\n",
    "while t < tfinal:\n",
    "    u = upwind @ u\n",
    "    hist.append((t, u))\n",
    "    t += dt\n",
    "hist = numpy.asarray(hist)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "for t, hist_ in hist[::len(hist)//10]:\n",
    "    pyplot.plot(x, hist_, label=f\"$t = {t:3.1f}$\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "1. The \"downwind\" discretisation is unstable (blowing up near the boundary). The upwind version is stable, if we have a small enough time step. This makes physical sense, because the downwind discretisation is trying to obtain information \"from the future\".\n",
    "\n",
    "2. The upwind discretisation is *dissipative*: the correct physical solution is just to transport the initial condition to the right, but we see that the peak spreads and flattens.\n",
    "\n",
    "3. This also occurs with sharp fronts (e.g. transporting a hat function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
