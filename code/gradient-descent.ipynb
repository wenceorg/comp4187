{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Forms and Steepest Descent\n",
    "\n",
    "A quadratic form is a scalar, quadratic function of a vector of the form:\n",
    "$$f (x) = \\frac{1}{2} x^T Ax − b^T x + c,\\text{ where }A = A^T$$\n",
    "\n",
    "![quad](quad.png)\n",
    "\n",
    "The gradient of a quadratic form is defined as $$ f^\\prime(x) = \\left( \\begin{array}{c}                       \\frac{\\partial}{\\partial x_1} f(x) \\\\                       \\vdots \\\\                       \\frac{\\partial}{\\partial x_n} f(x) \\\\                   \\end{array} \\right)$$\n",
    "\n",
    "So to compute the derivative we simply insert our definition of\n",
    "$$f(x) = \\frac{1}{2} x^T A x - b^T x + c.$$ \n",
    "Then $$f^\\prime(x) = Ax - b$$\n",
    "\n",
    "We can now see that $f^\\prime(x) = 0 \\quad\\Leftrightarrow\\quad Ax - b = 0 \n",
    "         \\quad\\Leftrightarrow\\quad Ax = b$\n",
    "\n",
    "\n",
    "This means that $Ax = b$ is equivalent to a minimisation problem. This minimisation problem is well-posed, i.e. has a unique minimum only if $A$ is positive definite. How do we solve that minimisation problem?\n",
    "\n",
    "### Direction of Steepest Descent\n",
    "\n",
    "The gradient $f^\\prime(x)$ gives us the direction of steepest ascent. Since we know that \n",
    "$$f^\\prime(x) = Ax - b = -r,$$ \n",
    "with $r$ being the residual $r = b-Ax$, we get that the residual $r$ is the direction of steepest descent.\n",
    "\n",
    "Let us set up a simple finite difference problem with a Poisson matrix and a random right hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import diags\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.close(fig)\n",
    "\n",
    "N=2\n",
    "h=0.25\n",
    "\n",
    "dat = np.linspace(-20,20,100); x, y = np.meshgrid(dat,dat); xv = np.array([x.reshape(-1), y.reshape(-1)]).T\n",
    "\n",
    "diagonals = [-np.ones(N-1), 2*np.ones(N), -np.ones(N-1)]\n",
    "A = diags(diagonals, [-1, 0, 1]).toarray() #Poisson matrix\n",
    "b = np.matlib.rand(N,1)                   # random right hand side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.dot(np.dot(x.T,A),x) - np.dot(b.T,x)\n",
    "\n",
    "h = np.array([f(xval) for xval in xv ]).reshape(100,100)\n",
    "plt.contour(x, y, h,15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_f(x): # derivative\n",
    "    return np.dot(A,x.T) - b.T\n",
    "\n",
    "dat = np.linspace(-5,5,8); x, y = np.meshgrid(dat,dat); xv = np.array([x.reshape(-1), y.reshape(-1)]).T\n",
    "u = np.array([derivative_f(xval) for xval in xv ])[:,:,0]\n",
    "v = np.array([derivative_f(xval) for xval in xv ])[:,:,1]\n",
    "plt.quiver(x,y, u,v);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving linear systems via Minimum Search\n",
    "\n",
    "The basic idea is to find the minimum by moving into direction of steepest descent.\n",
    "The most simple scheme is: $$x^{(i+1)} = x^{(i)} + \\alpha r^{(i)}$$\n",
    "where leaving $\\alpha$ constant gives us a Richardson iteration (usually considered as a relaxation method). Choosing an $\\alpha$ such that we move to lowest point in that direction gives us the steepest descent algorithm.\n",
    "\n",
    "These types of algorithms are used in many different areas, work through minimisation.ipynb for some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Descent – find an optimal $\\alpha$\n",
    "\n",
    "We are performing a line search along the line: \n",
    "$$x^{(1)} = x^{(0)} + \\alpha r^{(0)}.$$\n",
    "We want to choose $\\alpha$ such that  $f( x^{(1)} )$ is minimal, ie. such that $$\\frac{\\partial}{\\partial\\alpha} f (x^{(1)}) = 0$$\n",
    "If we use chain rule, we get: $$ \\frac{\\partial}{\\partial\\alpha} f (x^{(1)})  = f^\\prime (x^{(1)})^{\\!T} \\frac{\\partial}{\\partial\\alpha} x^{(1)}           = f^\\prime (x^{(1)})^{\\!T} r^{(0)}$$\n",
    "Remember $f^\\prime (x^{(1)}) = -r^{(1)}$, thus: $$             - \\left( r^{(1)} \\right)^{\\!T} r^{(0)} \\stackrel{!}{=} 0. $$\n",
    "This means that $f^\\prime (x^{(1)}) = -r^{(1)}$ should be orthogonal to $r^{(0)}$\n",
    "\n",
    "$$ \\left( r^{(1)} \\right)^{\\!T} r^{(0)}        = \\left( b - A x^{(1)} \\right)^{\\!T} r^{(0)} = 0 $$\n",
    "$$ \\left( b - A ( x^{(0)} + \\alpha r^{(0)} ) \\right)^{\\!T} r^{(0)} = 0 $$\n",
    "$$ \\left( b - A x^{(0)} \\right)^{\\!T} r^{(0)} - \\alpha \\left( A r^{(0)} \\right)^{\\!T} r^{(0)} = 0 $$\n",
    "$$ \\left( r^{(0)} \\right)^{\\!T} r^{(0)} - \\alpha \\left( r^{(0)} \\right)^{\\!T} A r^{(0)} = 0 $$\n",
    "  \n",
    " Solving for $\\alpha$ gives:\n",
    " $$       \\alpha  = \\frac{ \\left( r^{(0)} \\right)^{\\!T} r^{(0)} }{ \\left( r^{(0)} \\right)^{\\!T} A r^{(0)} }$$\n",
    " \n",
    "Using the value of $\\alpha$ gives us the classical steepest descent algorithm.\n",
    "\n",
    "#### Steepest Descent -- Algorithm\n",
    "\n",
    "1. $ r^{(i)} = b - A x^{(i)} $\n",
    "2. $ \\displaystyle \\alpha_i = \\frac{ \\left( r^{(i)} \\right)^{\\!T} r^{(i)} }  { \\left( r^{(i)} \\right)^{\\!T} A r^{(i)} } $\n",
    "3. $ x^{(i+1)} = x^{(i)} + \\alpha_i r^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,0] = 0.5; A[1,0]=0.0;A[0,1]=0.0; A[1,1]=2.5;b[0]=0.0;b[1]=0.0\n",
    "\n",
    "ax.clear()\n",
    "dat = np.linspace(-2.1,2.1,100);\n",
    "x, y = np.meshgrid(dat,dat); xv = np.array([x.reshape(-1), y.reshape(-1)]).T    \n",
    "h = np.array([f(xval) for xval in xv]).reshape(100,100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(num_it):\n",
    "    x = np.array([2.0, 2./5.]).reshape(-1,1) # initial guess\n",
    "    steps = x\n",
    "    for i in range(0,num_it):\n",
    "        r = b - np.dot(A,x).reshape(-1,1)             # compute residual\n",
    "        alpha = np.dot(r.T,r)/np.dot(r.T,np.dot(A,r)) # compute step size\n",
    "        x = x + np.multiply(alpha,r) # update\n",
    "        steps = np.vstack((steps, x))\n",
    "    return np.array(steps).reshape(num_it+1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(x, y, h, 30);\n",
    "steps = steepest_descent(6)\n",
    "plt.plot(steps.T[0], steps.T[1], \"x-\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "This method has a very slow convergence rate, we need many iterations of the method to reach the solution. Each iteration however can be computed in $O(N)$ since we only need matrix-vector products.\n",
    "A detailed analysis reveals: $$ \\left\\| e^{(i)} \\right\\|_A \\le           \\left( \\frac{ \\kappa -1 }{ \\kappa + 1 } \\right)^i          \\left\\| e^{(0)} \\right\\|_A$$\n",
    " with condition number $$\\kappa = \\lambda_{\\text{max}} / \\lambda_{\\text{min}},$$ where $\\lambda$ are the largest and smallest eigenvalues of $A$ respectively. For positive definite $A$ this are always larger than zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
